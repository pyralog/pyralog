# Pyralog Architecture

This document provides a deep dive into Pyralog's architecture, design decisions, and implementation details.

## Table of Contents

1. [Overview](#overview)
2. [Core Components](#core-components)
3. [Storage Engine](#storage-engine)
4. [Consensus Protocol](#consensus-protocol)
5. [Replication System](#replication-system)
6. [Network Protocol](#network-protocol)
7. [Performance Optimizations](#performance-optimizations)
8. [Failure Scenarios](#failure-scenarios)
9. [Scalability](#scalability)
10. [Monitoring and Observability](#monitoring-and-observability)
11. [Conclusion](#conclusion)

## Overview

Pyralog is a distributed log system designed for:
- **High throughput**: Millions of writes per second
- **Low latency**: Sub-millisecond write latencies
- **Strong durability**: Configurable replication and persistence
- **High availability**: Tolerates node failures
- **Horizontal scalability**: Add nodes to increase capacity

## Core Components

### 1. pyralog-core

Provides fundamental abstractions:

```
pyralog-core/
â”œâ”€â”€ error.rs          # Error types
â”œâ”€â”€ log.rs            # Log metadata
â”œâ”€â”€ offset.rs         # Offset types
â”œâ”€â”€ partition.rs      # Partition types
â”œâ”€â”€ record.rs         # Record and batch types
â””â”€â”€ traits.rs         # Core traits
```

**Key Types:**
- `LogOffset`: 64-bit offset in a log
- `Record`: Single log record with key, value, headers
- `RecordBatch`: Batch of records for efficient I/O
- `LogId`: Namespaced log identifier
- `PartitionId`: Partition identifier within a log

### 2. pyralog-storage

High-performance storage engine:

```
pyralog-storage/
â”œâ”€â”€ segment.rs        # Segment file management
â”œâ”€â”€ index.rs          # Offset index
â”œâ”€â”€ log_storage.rs    # Main storage interface
â”œâ”€â”€ write_cache.rs    # Write caching
â””â”€â”€ tiered.rs         # Tiered storage
```

**Design Principles:**
- Log-structured storage
- Sequential writes for performance
- Sparse indexes for fast lookups
- Memory-mapped I/O for zero-copy reads

### 3. pyralog-consensus

Raft-based consensus protocol:

```
pyralog-consensus/
â”œâ”€â”€ raft.rs           # Main Raft implementation
â”œâ”€â”€ state.rs          # Node state management
â”œâ”€â”€ rpc.rs            # RPC types
â”œâ”€â”€ election.rs       # Leader election
â””â”€â”€ log.rs            # Persistent log
```

**Responsibilities:**
- Cluster membership
- Leader election
- Metadata replication
- Configuration changes

### 4. pyralog-replication

Flexible quorum-based replication:

```
pyralog-replication/
â”œâ”€â”€ quorum.rs         # Quorum configuration
â”œâ”€â”€ copyset.rs        # CopySet selection
â”œâ”€â”€ replicator.rs     # Replication manager
â””â”€â”€ sync.rs           # Synchronization
```

**Features:**
- Configurable quorums
- CopySet replication
- ISR tracking
- Replication lag monitoring

### 5. pyralog-protocol

Protocol abstraction layer:

```
pyralog-protocol/
â”œâ”€â”€ api.rs            # API types
â”œâ”€â”€ partitioner.rs    # Partitioning strategies
â”œâ”€â”€ kafka.rs          # Kafka compatibility
â”œâ”€â”€ request.rs        # Request wire format
â””â”€â”€ response.rs       # Response wire format
```

## Storage Engine

### Segment-Based Storage

Data is organized into segments:

```
log-namespace/
â””â”€â”€ log-name/
    â””â”€â”€ partition-0/
        â”œâ”€â”€ 00000000000000000000.log      # Segment
        â”œâ”€â”€ 00000000000000000000.index    # Index
        â”œâ”€â”€ 00000000000001000000.log      # Next segment
        â””â”€â”€ 00000000000001000000.index    # Next index
```

**Segment Properties:**
- Fixed maximum size (default: 1GB)
- Immutable once full
- Can be memory-mapped for reads
- Atomic writes

### Index Structure

Sparse index for fast offset lookups:

```
Index Entry: [Offset (8 bytes)][Position (8 bytes)][Size (4 bytes)]
```

**Properties:**
- Not every record is indexed (sparse)
- Typically one entry per 4KB
- Entire index loaded in memory
- Binary search for lookups

### Write Cache

In-memory write buffer:

```
Write Cache
â”œâ”€â”€ Buffer: VecDeque<Record>
â”œâ”€â”€ Total Size: usize
â”œâ”€â”€ Last Flush: Instant
â””â”€â”€ Config
    â”œâ”€â”€ Max Size: 16MB
    â””â”€â”€ Max Time: 10ms
```

**Benefits:**
- Reduced write latency
- Batch multiple writes
- Configurable durability/latency tradeoff

## Consensus Protocol

### Raft Cluster Topology

**Key Question**: Do we need one global Raft cluster or per-partition Raft clusters?

**Answer**: Pyralog uses **BOTH** (dual Raft clusters):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dual Raft Cluster Architecture                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  1. GLOBAL RAFT CLUSTER (Metadata)                         â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚     All nodes participate                                  â”‚
â”‚     Purpose: Cluster-wide operations                       â”‚
â”‚                                                            â”‚
â”‚     Operations:                                            â”‚
â”‚     âœ“ Cluster membership changes                          â”‚
â”‚     âœ“ Partition creation/deletion                         â”‚
â”‚     âœ“ CopySet assignments (per-partition mode)            â”‚
â”‚     âœ“ Configuration changes                               â”‚
â”‚                                                            â”‚
â”‚     Example:                                               â”‚
â”‚     [N1, N2, N3, N4, N5] â†’ One Raft group                 â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  2. PER-PARTITION RAFT CLUSTERS (Epochs)                   â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚     Only partition replicas participate                    â”‚
â”‚     Purpose: Partition-specific operations                 â”‚
â”‚                                                            â”‚
â”‚     Operations:                                            â”‚
â”‚     âœ“ Epoch activation (leader election for partition)    â”‚
â”‚     âœ“ Epoch sealing (leadership transfer)                 â”‚
â”‚     âœ“ Partition-level failover                            â”‚
â”‚                                                            â”‚
â”‚     Example:                                               â”‚
â”‚     Partition 0: [N1, N2, N3] â†’ Raft group for P0        â”‚
â”‚     Partition 1: [N2, N3, N4] â†’ Raft group for P1        â”‚
â”‚     Partition 2: [N3, N4, N5] â†’ Raft group for P2        â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Dual Clusters?**

```
Scalability Trade-off:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Single Global Raft Only:
  âœ… Simple architecture
  âœ… One consensus group to manage
  âŒ Bottleneck: All epoch changes through one log
  âŒ Contention: 10K partitions = 10K ops/sec to global Raft
  âŒ Slow failover: Global Raft must process all partitions

Dual Raft (Global + Per-Partition):
  âœ… Parallel: Partition failovers are independent
  âœ… Fast: Epoch change only needs partition replicas
  âœ… Scalable: 10K partitions = 10K independent Raft groups
  âš ï¸  Complex: Each node in multiple Raft groups
  âš ï¸  Overhead: More Raft state to manage
```

### Node Membership in Raft Clusters

Each node is a member of **1 global + N partition Raft clusters**:

```rust
pub struct NodeRaftMembership {
    node_id: NodeId,
    
    // 1. Global cluster (always)
    global_raft: Arc<RaftNode>,
    
    // 2. Per-partition clusters (for partitions this node replicates)
    partition_rafts: HashMap<PartitionId, Arc<RaftNode>>,
}

// Example: Node 2 in 5-node cluster
// Node 2 is a member of:
//   - Global Raft: [N1, N2, N3, N4, N5]
//   - Partition 0 Raft: [N1, N2, N3]  (N2 is replica)
//   - Partition 1 Raft: [N2, N3, N4]  (N2 is replica)
//   - Partition 5 Raft: [N2, N4, N5]  (N2 is replica)
//
// Total: 1 global + 3 partition Raft groups = 4 Raft instances
```

### Raft State Machine

```
                    Follower
                       â”‚
                       â”‚ election timeout
                       â–¼
                   Candidate
                       â”‚
                       â”‚ receives votes from majority
                       â–¼
                     Leader
                       â”‚
                       â”‚ discovers higher term
                       â–¼
                    Follower
```

### Which Raft Cluster Does What?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Operation Flow: Global vs Per-Partition Raft              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  CLUSTER-WIDE OPERATIONS â†’ Global Raft                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚                                                             â”‚
â”‚  1. Add Node to Cluster                                     â”‚
â”‚     Client â†’ Any Node â†’ Propose to Global Raft             â”‚
â”‚     Global Raft commits â†’ All nodes updated                â”‚
â”‚                                                             â”‚
â”‚  2. Create New Partition                                    â”‚
â”‚     Admin API â†’ Global Raft                                â”‚
â”‚     Assigns partition ID, initial CopySet                  â”‚
â”‚     Creates per-partition Raft group                       â”‚
â”‚                                                             â”‚
â”‚  3. Reassign CopySet (rebalancing)                          â”‚
â”‚     Admin API â†’ Global Raft                                â”‚
â”‚     Updates CopySet metadata                               â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  PARTITION-SPECIFIC OPERATIONS â†’ Per-Partition Raft         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚                                                             â”‚
â”‚  1. Leader Election for Partition                           â”‚
â”‚     Partition 0's Raft: [N1, N2, N3]                       â”‚
â”‚     N1 fails â†’ N2 or N3 elected leader                     â”‚
â”‚     Only involves partition replicas!                      â”‚
â”‚                                                             â”‚
â”‚  2. Epoch Activation                                        â”‚
â”‚     New leader N2 for Partition 0                          â”‚
â”‚     N2 proposes epoch change to P0's Raft                  â”‚
â”‚     Only [N1, N2, N3] vote (fast!)                         â”‚
â”‚                                                             â”‚
â”‚  3. Epoch Sealing                                           â”‚
â”‚     Leader N2 seals old epoch                              â”‚
â”‚     Proposes to P0's Raft                                  â”‚
â”‚     Committed when majority of [N1, N2, N3] ack            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Concrete Example: Node Failure**

```
Initial State:
  Global Raft: [N1, N2, N3, N4, N5]
  Partition 0: [N1, N2, N3] â†’ Leader: N1
  Partition 1: [N2, N3, N4] â†’ Leader: N2

N1 Fails:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Global Raft: Detects N1 down (heartbeat timeout)
   â†’ Updates cluster membership (optional, for permanent failure)
   â†’ No impact on other partitions!

2. Partition 0 Raft: [N1, N2, N3] detects leader failure
   â†’ Election among [N2, N3] (N1 doesn't participate)
   â†’ N2 becomes leader for Partition 0
   â†’ N2 proposes new epoch to Partition 0 Raft
   â†’ Committed when N2 + N3 agree (majority of 2/3)
   
   This is FAST because:
   âœ… Only 3 nodes involved (not all 5)
   âœ… Parallel with other partitions
   âœ… No global bottleneck

3. Partition 1: Unaffected! 
   â†’ N2 is still leader
   â†’ Continues normal operation
```

### Benefits of Dual Raft

```
Parallelism:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1000 partitions fail over simultaneously:
  
  Single Global Raft:
    1000 epoch changes â†’ Global Raft log
    Sequential processing â†’ Slow!
    Latency: 1000 Ã— 10ms = 10 seconds âŒ
  
  Per-Partition Raft:
    1000 independent Raft groups
    Parallel processing â†’ Fast!
    Latency: 10ms (same as one partition) âœ…
```

### Log Replication Flow

```
Leader                      Follower
  â”‚                            â”‚
  â”‚â”€â”€â”€â”€ AppendEntries â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚    (entries, commitIndex)  â”‚
  â”‚                            â”‚
  â”‚                            â”‚ Apply entries
  â”‚                            â”‚ Update commitIndex
  â”‚                            â”‚
  â”‚â—„â”€â”€â”€â”€â”€ Success â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚    (matchIndex)            â”‚
  â”‚                            â”‚
  â”‚ Update commitIndex         â”‚
  â”‚                            â”‚
```

### Election Process

1. **Follower timeout**: No heartbeat from leader
2. **Become candidate**: Increment term, vote for self
3. **Request votes**: Send RequestVote RPC to all peers
4. **Collect votes**: Wait for majority
5. **Become leader**: If majority votes received
6. **Send heartbeats**: Establish authority

### Resource Management: Multiple Raft Groups

**Challenge**: Each node participates in 1 global + N partition Raft groups.

**Resource Usage Per Raft Group:**

```rust
pub struct RaftGroupResources {
    // Memory
    log_entries: Vec<LogEntry>,      // ~100 bytes per entry
    state: NodeState,                 // ~1 KB
    
    // Disk
    raft_log_file: File,              // Persistent log
    snapshot: Option<File>,           // Periodic snapshots
    
    // Network
    heartbeat_interval: Duration,     // 100ms default
    election_timeout: Duration,       // 300ms default
}
```

**Overhead Calculation:**

```
Example: 5-node cluster, RF=3, 1000 partitions

Per Node:
  Global Raft: 1 group
  Partition Rafts: ~600 groups (each node in ~60% of partitions)
  Total: ~601 Raft groups per node

Memory per Raft group: ~10 KB (state + recent entries)
Total memory: 601 Ã— 10 KB = ~6 MB âœ… (negligible!)

Disk per Raft group: ~1 MB (log + snapshots)
Total disk: 601 Ã— 1 MB = ~600 MB âœ… (acceptable)

Network: 601 Ã— 2 heartbeats/sec = ~1200 msgs/sec âœ… (fine)
```

**Optimization: Raft Multiplexing**

```rust
// Batch heartbeats for multiple Raft groups
pub struct MultiRaftManager {
    groups: HashMap<PartitionId, RaftNode>,
    
    pub async fn tick(&self) {
        // Single timer for all groups
        for (partition_id, raft) in &self.groups {
            raft.tick();
        }
    }
    
    pub async fn send_batched_heartbeats(&self, peer: NodeId) {
        // Batch heartbeats for all Raft groups to same peer
        let mut batch = Vec::new();
        for (partition_id, raft) in &self.groups {
            if raft.is_leader() {
                batch.push(HeartbeatMsg {
                    partition_id: *partition_id,
                    term: raft.current_term(),
                    commit_index: raft.commit_index(),
                });
            }
        }
        
        // Send one message instead of 600!
        self.send_to_peer(peer, batch).await;
    }
}
```

**Result**: 1000 partitions managed efficiently with minimal overhead! âœ…

## Replication System

### CopySet Selection Strategies

Pyralog supports **two CopySet selection strategies**, configurable based on your needs:

#### Strategy 1: Per-Partition CopySet (Kafka-style)

**Fixed CopySet per partition:**

```rust
pub struct PartitionCopySet {
    partition_id: PartitionId,
    nodes: Vec<NodeId>,  // Fixed: [N1, N2, N3]
}

// All records in partition 0 â†’ Always [N1, N2, N3]
// All records in partition 1 â†’ Always [N2, N3, N4]
```

**Advantages:**
- âœ… Simpler to implement
- âœ… Easier to reason about
- âœ… Faster lookups (cached per partition)
- âœ… Good for small clusters (< 10 nodes)

**Disadvantages:**
- âŒ Less uniform load distribution
- âŒ Hot partitions still overload same nodes
- âŒ Fixed replica sets

**Best for:**
- Smaller deployments
- Simpler operations
- Kafka-compatible behavior

#### Strategy 2: Per-Record CopySet (LogDevice-style)

**Dynamic CopySet per record/batch:**

```rust
pub struct RecordCopySet {
    lsn: u64,  // Log Sequence Number (epoch + offset)
    nodes: Vec<NodeId>,  // Calculated: hash(lsn) â†’ [N1, N5, N7]
}

// Record @ LSN 1000 â†’ hash(1000) â†’ [N1, N5, N7]
// Record @ LSN 1001 â†’ hash(1001) â†’ [N2, N3, N6]
// Record @ LSN 1002 â†’ hash(1002) â†’ [N1, N4, N8]
```

**Key Innovation: Leader as Coordinator**

With per-record CopySet, the **leader doesn't need to store data locally**:

```
Traditional (Leader Stores Data):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client â†’ Leader â†’ Write to local disk
                â†’ Replicate to followers
                
Leader role: Storage + Coordinator
Leader disk: Heavy I/O (ALL partition data)

Per-Record CopySet (Leader Coordinates Only):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client â†’ Leader â†’ Calculate CopySet
                â†’ Send to storage nodes
                
Leader role: Coordinator ONLY
Leader disk: Metadata only (epochs, offsets)
Reduction: 99%+ less leader I/O! ğŸš€
```

**Advantages:**
- âœ… Maximum load distribution
- âœ… Hot keys don't overload same nodes
- âœ… Better fault tolerance
- âœ… No metadata storage needed (deterministic)
- âœ… **Leader can be disk-free** (just coordinates)
- âœ… **Leader can handle 10x more partitions** (no storage overhead)
- âœ… **Disk failure doesn't affect leadership** (no local data)

**Disadvantages:**
- âŒ More complex implementation
- âŒ Readers must calculate CopySet
- âŒ Slightly more CPU for hash calculation
- âŒ Can't read directly from leader (unless hybrid mode)

**Best for:**
- Large clusters (10+ nodes)
- Uneven key distributions
- Maximum performance
- High partition count per node

### Configuration

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CopySetStrategy {
    /// Fixed CopySet per partition
    PerPartition,
    
    /// Dynamic CopySet per record (LogDevice-style)
    PerRecord {
        /// Seed for deterministic selection
        seed: u64,
        
        /// Should leader store data locally?
        /// false = Leader is pure coordinator (LogDevice-style)
        /// true = Leader also stores data (hybrid mode)
        leader_stores_data: bool,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReplicationConfig {
    pub replication_factor: usize,
    pub write_quorum: usize,
    pub read_quorum: usize,
    
    /// CopySet selection strategy
    pub copyset_strategy: CopySetStrategy,
}
```

**Configuration file:**

```toml
[replication]
replication_factor = 3
write_quorum = 2
read_quorum = 2

# Option 1: Per-partition (simpler)
copyset_strategy = "PerPartition"

# Option 2: Per-record with leader as coordinator (maximum performance)
[replication.copyset_strategy]
type = "PerRecord"
seed = 42
leader_stores_data = false  # Leader disk-free! ğŸš€

# Option 3: Per-record hybrid (leader also stores)
[replication.copyset_strategy]
type = "PerRecord"
seed = 42
leader_stores_data = true   # Leader can serve reads
```

### Implementation: Per-Partition CopySet

```rust
pub struct PartitionCopySetSelector {
    // Stored in metadata (Raft + RocksDB)
    assignments: Arc<RwLock<HashMap<PartitionId, Vec<NodeId>>>>,
    replication_factor: usize,
}

impl PartitionCopySetSelector {
    pub fn select(&self, partition: PartitionId) -> Vec<NodeId> {
        // Simple lookup - O(1)
        self.assignments
            .read()
            .get(&partition)
            .cloned()
            .unwrap_or_default()
    }
    
    pub async fn assign(&self, partition: PartitionId) -> Result<Vec<NodeId>> {
        // Round-robin or random selection
        let nodes = self.select_nodes_for_partition(partition);
        
        // Store in metadata via Raft consensus
        self.propose_assignment(partition, nodes.clone()).await?;
        
        // Cache locally
        self.assignments.write().insert(partition, nodes.clone());
        
        Ok(nodes)
    }
}
```

### Implementation: Per-Record CopySet

```rust
pub struct RecordCopySetSelector {
    nodes: Vec<NodeId>,
    replication_factor: usize,
    seed: u64,
}

impl RecordCopySetSelector {
    pub fn select(&self, lsn: u64) -> Vec<NodeId> {
        // Deterministic selection based on LSN
        // NO storage needed - pure function!
        
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        lsn.hash(&mut hasher);
        self.seed.hash(&mut hasher);
        let hash = hasher.finish();
        
        // Select RF unique nodes using hash
        let mut selected = Vec::with_capacity(self.replication_factor);
        let mut offset = hash as usize;
        
        while selected.len() < self.replication_factor {
            let idx = offset % self.nodes.len();
            let node = self.nodes[idx];
            
            if !selected.contains(&node) {
                selected.push(node);
            }
            offset += 1;
        }
        
        selected
    }
    
    // NO storage or consensus needed!
    // Any node can calculate the same CopySet for a given LSN
}
```

### Write Path with Both Strategies

```rust
pub struct Replicator {
    strategy: CopySetStrategy,
    partition_selector: Option<PartitionCopySetSelector>,
    record_selector: Option<RecordCopySetSelector>,
    node_id: NodeId,
}

impl Replicator {
    pub async fn replicate(&self, record: Record) -> Result<()> {
        // Select CopySet based on strategy
        let (copyset, include_leader) = match &self.strategy {
            CopySetStrategy::PerPartition => {
                // Lookup partition's fixed CopySet
                let selector = self.partition_selector.as_ref().unwrap();
                let copyset = selector.select(record.partition_id);
                (copyset, true)  // Leader always stores data
            }
            
            CopySetStrategy::PerRecord { seed, leader_stores_data } => {
                // Calculate CopySet from LSN
                let selector = self.record_selector.as_ref().unwrap();
                let lsn = record.epoch_offset.as_u64();
                let mut copyset = selector.select(lsn);
                
                // If leader_stores_data=true, ensure leader is in CopySet
                if *leader_stores_data && !copyset.contains(&self.node_id) {
                    copyset.push(self.node_id);
                }
                
                (copyset, *leader_stores_data)
            }
        };
        
        // Write to local storage (if configured)
        if include_leader && copyset.contains(&self.node_id) {
            self.local_storage.append(record.clone()).await?;
        }
        
        // Send to remote nodes in CopySet
        let futures: Vec<_> = copyset.iter()
            .filter(|&&node| node != self.node_id)  // Skip self if already written
            .map(|&node| self.send_to_node(node, record.clone()))
            .collect();
        
        // Wait for write quorum
        self.wait_for_quorum(futures).await
    }
}
```

**Leader as Pure Coordinator (leader_stores_data=false):**

```rust
// Leader (Sequencer) - Lightweight, no storage!
pub struct Sequencer {
    partition_id: PartitionId,
    current_epoch: Epoch,
    next_offset: AtomicU64,
    copyset_selector: RecordCopySetSelector,
    // NO local_storage field!
}

impl Sequencer {
    pub async fn handle_write(&self, record: Record) -> Result<LogOffset> {
        // 1. Assign LSN (metadata only, no disk I/O!)
        let epoch = self.current_epoch;
        let offset = LogOffset::new(
            self.next_offset.fetch_add(1, Ordering::SeqCst)
        );
        let lsn = EpochOffset::new(epoch, offset.as_u64()).as_u64();
        
        // 2. Calculate CopySet (pure function, deterministic)
        let copyset = self.copyset_selector.select(lsn);
        // â†’ [Node 3, Node 7, Node 9]
        
        // 3. Prepare record with LSN
        let mut record = record;
        record.epoch = epoch;
        record.offset = offset;
        
        // 4. Send directly to storage nodes (NOT local disk!)
        for node in copyset {
            self.send_to_storage_node(node, record.clone()).await?;
        }
        
        // 5. Wait for write quorum
        self.wait_for_quorum(copyset.len()).await?;
        
        // 6. Done! Leader never touched disk! âœ…
        Ok(offset)
    }
}
```

**Storage Node (Receives and Stores):**

```rust
// Storage node - Stores data selected by CopySet
pub struct StorageNode {
    node_id: NodeId,
    storage: LogStorage,
}

impl StorageNode {
    pub async fn handle_write(&self, record: Record) -> Result<()> {
        // Storage node writes to disk
        self.storage.append(record).await?;
        Ok(())
    }
}
```

### Read Path with Both Strategies

```rust
impl Reader {
    pub async fn read(&self, partition: PartitionId, offset: LogOffset) -> Result<Record> {
        // Find which nodes have this record
        let copyset = match &self.strategy {
            CopySetStrategy::PerPartition => {
                // Lookup partition's fixed CopySet
                self.partition_selector.select(partition)
            }
            
            CopySetStrategy::PerRecord { seed } => {
                // Calculate from LSN (epoch + offset)
                let lsn = self.calculate_lsn(partition, offset)?;
                self.record_selector.select(lsn)
            }
        };
        
        // Try reading from any node in the CopySet
        for node in copyset {
            if let Ok(record) = self.try_read_from(node, partition, offset).await {
                return Ok(record);
            }
        }
        
        Err(PyralogError::RecordNotFound(offset))
    }
}
```

### Leader as Coordinator: Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Traditional: Leader Stores All Partition Data         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Node 1 (Leader for P0):                                â”‚
â”‚    â”œâ”€ Sequencer (assigns offsets)                       â”‚
â”‚    â”œâ”€ Local Storage (/segments/P0/)     â† Heavy I/O âš ï¸  â”‚
â”‚    â”‚  â””â”€ All records for partition 0                    â”‚
â”‚    â””â”€ Replicates to followers                           â”‚
â”‚                                                         â”‚
â”‚  Node 2, Node 3 (Followers):                            â”‚
â”‚    â””â”€ Receive replicas of partition 0                   â”‚
â”‚                                                         â”‚
â”‚  Problem: Leader disk is bottleneck                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Per-Record CopySet: Leader Coordinates Only           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Node 1 (Leader for P0):                                â”‚
â”‚    â”œâ”€ Sequencer (assigns LSN)                           â”‚
â”‚    â”œâ”€ NO local storage! âœ…                               â”‚
â”‚    â””â”€ Routes to storage nodes                           â”‚
â”‚         â”‚                                               â”‚
â”‚         â”œâ”€â”€â–º Record @ LSN 1000 â†’ CopySet [N3, N7, N9]  â”‚
â”‚         â”œâ”€â”€â–º Record @ LSN 1001 â†’ CopySet [N2, N5, N8]  â”‚
â”‚         â””â”€â”€â–º Record @ LSN 1002 â†’ CopySet [N4, N6, N7]  â”‚
â”‚                                                         â”‚
â”‚  Nodes 2-10 (Storage Nodes):                            â”‚
â”‚    â””â”€ Each stores subset of records                     â”‚
â”‚       Based on deterministic CopySet selection          â”‚
â”‚                                                         â”‚
â”‚  Result: Leader is lightweight! ğŸš€                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Leader Resource Usage:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Leader Disk I/O Comparison                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Per-Partition CopySet:                          â”‚
â”‚    Disk writes: 100 GB/hour (all data)          â”‚
â”‚    Disk reads: 50 GB/hour (serves reads)        â”‚
â”‚    Total I/O: 150 GB/hour âš ï¸                     â”‚
â”‚                                                  â”‚
â”‚  Per-Record CopySet (leader_stores_data=false):  â”‚
â”‚    Disk writes: 10 MB/hour (metadata only)      â”‚
â”‚    Disk reads: 5 MB/hour (metadata only)        â”‚
â”‚    Total I/O: 15 MB/hour âœ…                      â”‚
â”‚                                                  â”‚
â”‚  Reduction: 99.99% less I/O! ğŸ‰                  â”‚
â”‚                                                  â”‚
â”‚  Leader can handle:                              â”‚
â”‚    Before: 10-20 partitions                     â”‚
â”‚    After: 200-500 partitions                    â”‚
â”‚    Increase: 20x-50x! ğŸš€                         â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits for Large Deployments:**

```
Scenario: 1000 partitions, 20 nodes

Per-Partition CopySet:
  Each node leads: 50 partitions
  Each partition: 10 GB data
  Leader storage: 50 Ã— 10 GB = 500 GB per node
  Problem: Disk-bound! ğŸ’¥

Per-Record CopySet (coordinator mode):
  Each node leads: 50 partitions
  Each partition: Metadata only
  Leader storage: 50 Ã— 1 MB = 50 MB per node
  Solution: CPU-bound (better!) âœ…
  
  Actual data: Distributed across all 20 nodes
  Each node stores: ~1/3 of total data (RF=3)
  Balanced load across cluster!
```

### Load Distribution Comparison

```
Scenario: 10 nodes, 100 partitions, RF=3, 1M records

Per-Partition CopySet:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Partition 0 â†’ [N1, N2, N3]  (10K records)
Partition 1 â†’ [N2, N3, N4]  (10K records)
...
Partition 99 â†’ [N7, N8, N9] (10K records)

If partition 0 is hot (100K records):
  N1, N2, N3: 100K records each âš ï¸ 
  N4-N10: 10K records each
  
Imbalance: 10x difference!

Per-Record CopySet:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Record 1 â†’ hash(1) â†’ [N1, N3, N5]
Record 2 â†’ hash(2) â†’ [N2, N4, N7]
Record 3 â†’ hash(3) â†’ [N1, N6, N8]
...

Even if 100K records have same key:
  Each record gets different CopySet
  All nodes: ~30K records each âœ…
  
Imbalance: ~1.1x (much better!)
```

### When to Use Each Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Decision Matrix                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Use Per-Partition if:                                  â”‚
â”‚    â€¢ Cluster size < 10 nodes                            â”‚
â”‚    â€¢ Keys are well-distributed                          â”‚
â”‚    â€¢ Simplicity is priority                             â”‚
â”‚    â€¢ Kafka-compatible behavior needed                   â”‚
â”‚    â€¢ Lower partition count (< 100 partitions/node)      â”‚
â”‚    â€¢ Leader can handle storage load                     â”‚
â”‚                                                         â”‚
â”‚  Use Per-Record (leader_stores_data=false) if:          â”‚
â”‚    â€¢ Cluster size >= 10 nodes                           â”‚
â”‚    â€¢ Uneven key distribution / hot partitions           â”‚
â”‚    â€¢ High partition count (100s per node)               â”‚
â”‚    â€¢ Maximum performance needed                         â”‚
â”‚    â€¢ Large scale (billions of records)                  â”‚
â”‚    â€¢ Leader disk is bottleneck                          â”‚
â”‚    â€¢ Want to separate coordination from storage         â”‚
â”‚                                                         â”‚
â”‚  Use Per-Record (leader_stores_data=true) if:           â”‚
â”‚    â€¢ Want per-record distribution benefits              â”‚
â”‚    â€¢ But also want leader to serve reads                â”‚
â”‚    â€¢ Hybrid approach for migration                      â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Rule of thumb:**
- **< 10 nodes**: Start with per-partition (simpler)
- **10-50 nodes**: Per-record with leader storage (hybrid)
- **50+ nodes**: Per-record coordinator-only (maximum scale)

### Migration Between Strategies

You can change strategies online:

```rust
// Start with per-partition
config.copyset_strategy = CopySetStrategy::PerPartition;

// Later, migrate to per-record for better distribution
config.copyset_strategy = CopySetStrategy::PerRecord { seed: 42 };

// Old records: Still use partition CopySet (backward compatible)
// New records: Use per-record CopySet
```

**Benefits:**
- âœ… Both strategies in one system
- âœ… Choose based on cluster size and workload
- âœ… Can migrate as you scale
- âœ… Best of both worlds!

### Summary: Three Configuration Modes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Mode Comparison                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  MODE 1: Per-Partition CopySet                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  copyset_strategy = "PerPartition"                         â”‚
â”‚                                                            â”‚
â”‚  âœ… Simplest implementation                                 â”‚
â”‚  âœ… Kafka-compatible                                        â”‚
â”‚  âœ… Leader serves reads                                     â”‚
â”‚  âŒ Fixed replica sets                                      â”‚
â”‚  âŒ Hot partitions overload nodes                           â”‚
â”‚  âŒ Leader stores all data                                  â”‚
â”‚                                                            â”‚
â”‚  Best for: Small clusters (< 10 nodes)                     â”‚
â”‚  Partitions/node: 10-20                                    â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  MODE 2: Per-Record with Leader Storage (Hybrid)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  [replication.copyset_strategy]                            â”‚
â”‚  type = "PerRecord"                                        â”‚
â”‚  seed = 42                                                 â”‚
â”‚  leader_stores_data = true                                 â”‚
â”‚                                                            â”‚
â”‚  âœ… Better load distribution                                â”‚
â”‚  âœ… Leader serves reads                                     â”‚
â”‚  âœ… Hot keys don't overload                                 â”‚
â”‚  âš ï¸  Leader still stores data                               â”‚
â”‚                                                            â”‚
â”‚  Best for: Medium clusters (10-50 nodes)                   â”‚
â”‚  Partitions/node: 20-100                                   â”‚
â”‚                                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  MODE 3: Per-Record Coordinator-Only (Maximum Scale)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  [replication.copyset_strategy]                            â”‚
â”‚  type = "PerRecord"                                        â”‚
â”‚  seed = 42                                                 â”‚
â”‚  leader_stores_data = false                                â”‚
â”‚                                                            â”‚
â”‚  âœ… Maximum load distribution                               â”‚
â”‚  âœ… Leader disk-free (99%+ less I/O)                        â”‚
â”‚  âœ… Leader handles 20x-50x more partitions                  â”‚
â”‚  âœ… Separation of coordination and storage                  â”‚
â”‚  âŒ Can't read from leader                                  â”‚
â”‚  âŒ More complex read path                                  â”‚
â”‚                                                            â”‚
â”‚  Best for: Large clusters (50+ nodes)                      â”‚
â”‚  Partitions/node: 100-500                                  â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Scalability Progression:**

```
Start Small â†’ Grow Large â†’ Maximum Scale
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mode 1      â†’ Mode 2     â†’ Mode 3
Per-Partition  Hybrid       Coordinator

10 nodes    â†’ 30 nodes   â†’ 100 nodes
20 parts    â†’ 100 parts  â†’ 500 parts/node
Simple      â†’ Balanced   â†’ Maximum perf
```

### CopySet Storage

CopySets are **cluster metadata**, stored separately from log data:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CopySet Storage Architecture            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  1. RAFT LOG (Consensus)                        â”‚
â”‚     â”œâ”€ CopySet assignments proposed            â”‚
â”‚     â”œâ”€ Cluster votes on assignments            â”‚
â”‚     â””â”€ Committed to Raft log                   â”‚
â”‚                                                 â”‚
â”‚  2. METADATA STORE (Persistence)                â”‚
â”‚     â”œâ”€ RocksDB or similar KV store             â”‚
â”‚     â”œâ”€ Key: partition_id                       â”‚
â”‚     â””â”€ Value: CopySetMetadata                  â”‚
â”‚                                                 â”‚
â”‚  3. IN-MEMORY CACHE (Performance)               â”‚
â”‚     â”œâ”€ ClusterManager holds map                â”‚
â”‚     â”œâ”€ Fast lookups during writes              â”‚
â”‚     â””â”€ Refreshed from metadata store           â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Structure:**

```rust
// Stored in metadata store
pub struct CopySetMetadata {
    pub partition_id: PartitionId,
    pub nodes: Vec<NodeId>,           // [Node1, Node2, Node3]
    pub leader: NodeId,                // Current leader
    pub created_at: SystemTime,
    pub last_modified: SystemTime,
}

// In-memory representation
pub struct ClusterManager {
    // Fast lookup: partition -> copyset
    copyset_cache: Arc<RwLock<HashMap<PartitionId, CopySetMetadata>>>,
    
    // Persistent store
    metadata_store: Arc<MetadataStore>,
    
    // Raft for consensus
    raft: Arc<RaftNode>,
}
```

**File System Layout:**

```
/var/lib/pyralog/
â”œâ”€â”€ raft/
â”‚   â””â”€â”€ raft.log              â† CopySet changes in Raft log
â”œâ”€â”€ metadata/
â”‚   â””â”€â”€ rocksdb/
â”‚       â””â”€â”€ copysets/         â† Persistent CopySet storage
â”‚           â”œâ”€â”€ partition_0   â†’ [N1, N2, N4]
â”‚           â”œâ”€â”€ partition_1   â†’ [N2, N3, N5]
â”‚           â””â”€â”€ partition_2   â†’ [N1, N3, N6]
â””â”€â”€ segments/
    â””â”€â”€ partition_0/          â† Actual log data
        â”œâ”€â”€ 00000000.log
        â””â”€â”€ 00000000.index
```

**Key Characteristics:**

```
CopySets (Metadata):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Size: O(partitions Ã— replication_factor)
      Small - hundreds of KB

Frequency: Changed rarely
          (only on rebalancing/failures)

Storage: Raft log + RocksDB
         Must be consistent across cluster

Access: Fast in-memory lookup
        Cached by ClusterManager

vs.

Log Records (Data):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Size: O(millions/billions of records)
      Large - terabytes

Frequency: Changed constantly
          (every write)

Storage: Segment files on disk
         Per-replica, eventually consistent

Access: Disk I/O or mmap
        Indexed lookups
```

**CopySet Assignment Flow:**

```
New Partition Created:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Leader proposes CopySet assignment
   Propose([Node1, Node3, Node5])
   
2. Raft consensus
   Majority votes â†’ Committed
   
3. Write to metadata store
   RocksDB: partition_id â†’ [N1, N3, N5]
   
4. Update in-memory cache
   ClusterManager.copyset_cache.insert(...)
   
5. All nodes see consistent CopySet
   Used for all writes to this partition
   
Duration: Once per partition creation
Cost: ~100ms (Raft consensus)
```

**CopySet Lookup During Write:**

```
Write arrives for Partition 0:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Leader checks cache (fast!)
   copyset = copyset_cache.get(partition_0)
   â†’ [Node1, Node2, Node4]
   
2. Replicate to these nodes
   Send to Node1, Node2, Node4 in parallel
   
3. No disk I/O for CopySet lookup
   Already in memory!
   
Duration: ~1 microsecond (hash map lookup)
Cost: Negligible
```

**Why Store in Raft?**

CopySets must be **strongly consistent** across the cluster:
- All nodes must agree on which nodes hold a partition
- Prevents split-brain (different nodes thinking different CopySets)
- Raft provides linearizable consensus
- Changes are rare, so Raft overhead is acceptable

**Comparison:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   What Gets Stored Where                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  Raft Log + Metadata Store:                     â”‚
â”‚    âœ“ CopySet assignments                        â”‚
â”‚    âœ“ Epoch changes                              â”‚
â”‚    âœ“ Cluster membership                         â”‚
â”‚    âœ“ Leader elections                           â”‚
â”‚    âœ— NOT log records (too many!)               â”‚
â”‚                                                 â”‚
â”‚  Segment Files (Log Storage):                   â”‚
â”‚    âœ“ Actual records                             â”‚
â”‚    âœ“ Record epochs (tagged on each record)     â”‚
â”‚    âœ“ Offsets                                    â”‚
â”‚    âœ— NOT CopySet info (would duplicate)        â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flexible Quorums

```
Quorum Configuration:
- Replication Factor: R
- Write Quorum: W
- Read Quorum: R

Constraint: W + R > RF (ensures overlap)
```

**Examples:**

| Config | R | W | Rd | Use Case |
|--------|---|---|-----|----------|
| Majority | 3 | 2 | 2 | Balanced |
| Write-Heavy | 3 | 1 | 3 | Low write latency |
| Read-Heavy | 3 | 3 | 1 | Low read latency |
| Strong Consistency | 3 | 3 | 3 | Maximum durability |

### ISR (In-Sync Replicas)

Track which replicas are up-to-date:

```
Partition 0:
- Leader: Node 1 (offset: 1000)
- ISR: [Node 1, Node 2, Node 3]
- Follower Offsets:
  - Node 2: 998 (in sync, lag < 1000)
  - Node 3: 995 (in sync, lag < 1000)
  - Node 4: 500 (out of sync, lag > 1000)
```

## Network Protocol

### Smart Client Architecture

Pyralog uses the **smart client pattern** where clients fetch metadata and connect directly to partition leaders, avoiding proxy hops:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Smart Client Flow                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  1. Bootstrap & Metadata Discovery             â”‚
â”‚     Client â†’ Any Server: MetadataRequest       â”‚
â”‚     Server â†’ Client: {                         â”‚
â”‚       partition_0: leader=Node5,               â”‚
â”‚       partition_1: leader=Node3,               â”‚
â”‚       partition_2: leader=Node1                â”‚
â”‚     }                                          â”‚
â”‚                                                â”‚
â”‚  2. Client Caches Topology                     â”‚
â”‚     partition_cache[0] = Node5                 â”‚
â”‚     partition_cache[1] = Node3                 â”‚
â”‚     partition_cache[2] = Node1                 â”‚
â”‚                                                â”‚
â”‚  3. Direct Write to Leader                     â”‚
â”‚     Client calculates: hash(key) % 3 = 0       â”‚
â”‚     Client connects directly to Node5          â”‚
â”‚     No proxy overhead! âœ…                      â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Direct connection (1 hop vs 2)
- âœ… Client-side load balancing
- âœ… No proxy bottleneck
- âœ… Scales with cluster size

**Metadata includes:**
- Partition â†’ Leader mapping
- Replica locations for reads
- ISR (In-Sync Replicas) status
- Node addresses and ports

For detailed flow and implementation, see [DATA_PATH.md](DATA_PATH.md#smart-client-architecture).

### Request/Response Flow

```
Client                      Server
  â”‚                           â”‚
  â”‚â”€â”€â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚    [records]              â”‚
  â”‚                           â”‚
  â”‚                           â”‚ Write to storage
  â”‚                           â”‚ Replicate
  â”‚                           â”‚
  â”‚â—„â”€â”€â”€ ProduceResponse â”€â”€â”€â”€â”€â”€â”‚
  â”‚    [offset]               â”‚
  â”‚                           â”‚
```

### Wire Format

```
Message Format:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Length (4B)    â”‚ Request ID (8B)  â”‚ Payload     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Payload (bincode serialized):
- Request Type
- Request Data
```

## Performance Optimizations

### 1. Zero-Copy I/O

```rust
// Memory-mapped file
let mmap = unsafe { Mmap::map(&file)? };

// Zero-copy read
let data = &mmap[offset..offset+length];
```

### 2. Batch Processing

```rust
// Batch multiple records
let batch = RecordBatch::new(base_offset, records);

// Single write operation
storage.append_batch(batch).await?;
```

### 3. Write Caching

```rust
// Add to cache (no disk I/O)
cache.push(record)?;

// Flush when needed
if cache.should_flush() {
    cache.drain_and_write().await?;
}
```

### 4. Async I/O

```rust
// Concurrent operations
let (r1, r2, r3) = tokio::join!(
    storage.append(rec1),
    storage.append(rec2),
    storage.append(rec3),
);
```

## Failure Scenarios

### Node Failure

```
Before:
Leader: Node 1
Followers: [Node 2, Node 3]

After Node 1 fails:
1. Followers detect missing heartbeats
2. Election timeout triggers
3. Node 2 becomes candidate
4. Node 2 wins election
5. Node 2 is new leader

Recovery time: ~300ms
```

### Network Partition

```
Partition scenario:
[Node 1] | [Node 2, Node 3]

With 3 nodes, majority is 2:
- Node 1 cannot form quorum (steps down)
- Nodes 2,3 can form quorum (elect new leader)

System continues operating on majority side
```

### Data Corruption

```
Detection:
1. CRC checksum on each batch
2. Verification on read
3. Replication checksum comparison

Recovery:
1. Detect corrupted segment
2. Request from replica
3. Rebuild from healthy copy
```

### Disk Failure

```
With tiered storage:
1. Detect disk failure
2. Mark node as degraded
3. Redirect reads to replicas
4. Background recovery from object storage
5. Rebuild local copy
```

## Scalability

### The Leader Bottleneck Problem

Pyralog uses a **leader-based architecture** where all writes for a partition must go through a single leader node. This creates a potential bottleneck:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Single Partition Write Path             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  All clients writing to Partition 0             â”‚
â”‚        â”‚         â”‚         â”‚                    â”‚
â”‚        â–¼         â–¼         â–¼                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚    â”‚   Leader Node 1          â”‚ â† BOTTLENECK!  â”‚
â”‚    â”‚   (Partition 0)          â”‚                â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚            â”‚ Replicate                          â”‚
â”‚       â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”                               â”‚
â”‚       â–¼    â–¼    â–¼                               â”‚
â”‚    Node2 Node3 Node4                            â”‚
â”‚   (Followers)                                   â”‚
â”‚                                                 â”‚
â”‚  Limit: Leader's CPU/Network/Disk              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this happens:**
- All writes must flow through the partition leader
- Leader assigns offsets (with epochs for efficiency)
- Leader coordinates replication
- Single point of serialization per partition

**But this is a deliberate trade-off** for strong consistency and ordering guarantees!

### Solution 1: Distributed Leadership via Partitioning

Pyralog distributes leadership across the cluster through partitioning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Distributed Leadership                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  16 Partitions, 4 Nodes                              â”‚
â”‚                                                      â”‚
â”‚  Node 1 leads: Partitions [0, 4, 8, 12]            â”‚
â”‚  Node 2 leads: Partitions [1, 5, 9, 13]            â”‚
â”‚  Node 3 leads: Partitions [2, 6, 10, 14]           â”‚
â”‚  Node 4 leads: Partitions [3, 7, 11, 15]           â”‚
â”‚                                                      â”‚
â”‚  Client A â”€â”€â†’ hash("key-1") % 16 = 0 â”€â”€â†’ Node 1    â”‚
â”‚  Client B â”€â”€â†’ hash("key-2") % 16 = 5 â”€â”€â†’ Node 2    â”‚
â”‚  Client C â”€â”€â†’ hash("key-3") % 16 = 10 â”€â”€â†’ Node 3   â”‚
â”‚  Client D â”€â”€â†’ hash("key-4") % 16 = 15 â”€â”€â†’ Node 4   â”‚
â”‚                                                      â”‚
â”‚  Result: Writes distributed across ALL nodes! âœ…    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Throughput scaling:**

```
Single partition:
  Leader throughput: 100K records/sec
  Total: 100K records/sec

16 partitions (4 nodes):
  16 leaders Ã— 100K = 1.6M records/sec
  Scaling: 16x âœ…

64 partitions (8 nodes):
  64 leaders Ã— 100K = 6.4M records/sec
  Scaling: 64x âœ…
```

### Solution 2: Read Scaling via Replicas

While writes must go through the leader, **reads can come from any replica**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Write vs Read Paths                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  WRITE PATH (must use leader):                  â”‚
â”‚  Client â”€â”€â†’ Leader â”€â”€â†’ Replicate â”€â”€â†’ Followers  â”‚
â”‚             Single                               â”‚
â”‚                                                  â”‚
â”‚  READ PATH (any replica):                        â”‚
â”‚  Client A â”€â”€â†’ Node 1 (any replica) â”            â”‚
â”‚  Client B â”€â”€â†’ Node 2 (any replica) â”œâ”€ Load      â”‚
â”‚  Client C â”€â”€â†’ Node 3 (any replica) â”‚  balanced! â”‚
â”‚  Client D â”€â”€â†’ Node 4 (any replica) â”˜            â”‚
â”‚                                                  â”‚
â”‚  Read throughput with RF=3:                      â”‚
â”‚    1 partition Ã— 3 replicas = 3x reads âœ…        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration:**

```rust
// Allow reads from any replica (eventual consistency)
config.read_policy = ReadPolicy::AnyReplica;

// Or require leader reads (strong consistency)
config.read_policy = ReadPolicy::LeaderOnly;

// Or require read quorum
config.read_policy = ReadPolicy::Quorum(2);
```

### Solution 3: CopySet Distribution

Pyralog uses **non-deterministic replica placement** to distribute load:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Traditional Replication (Bottleneck)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  All partitions use same replica set:            â”‚
â”‚    Partition 0: [Node 1, Node 2, Node 3]        â”‚
â”‚    Partition 1: [Node 1, Node 2, Node 3]        â”‚
â”‚    Partition 2: [Node 1, Node 2, Node 3]        â”‚
â”‚                                                  â”‚
â”‚  Problem: Nodes 1,2,3 always get all traffic!   â”‚
â”‚           Other nodes underutilized!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CopySet Replication (Pyralog) âœ…                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  Each partition uses different copysets:         â”‚
â”‚    Partition 0: [Node 1, Node 2, Node 4]        â”‚
â”‚    Partition 1: [Node 2, Node 3, Node 5]        â”‚
â”‚    Partition 2: [Node 1, Node 3, Node 6]        â”‚
â”‚    Partition 3: [Node 4, Node 5, Node 6]        â”‚
â”‚                                                  â”‚
â”‚  Result: Load spread across entire cluster! âœ…   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How These Solutions Complement Each Other

The three solutions work together to eliminate bottlenecks at different levels:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Problem Solved by Each Solution                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Solution 1: Partitioning                           â”‚
â”‚    Distributes WRITE LEADERSHIP                     â”‚
â”‚    â”œâ”€ Each partition has one leader                â”‚
â”‚    â”œâ”€ Leaders distributed across nodes             â”‚
â”‚    â””â”€ Avoids single leader bottleneck              â”‚
â”‚                                                     â”‚
â”‚  Solution 2: Read Replicas                          â”‚
â”‚    Distributes READ LOAD                            â”‚
â”‚    â”œâ”€ Clients can read from any replica            â”‚
â”‚    â”œâ”€ Multiplies read capacity by RF                â”‚
â”‚    â””â”€ Avoids read bottleneck                       â”‚
â”‚                                                     â”‚
â”‚  Solution 3: CopySet                                â”‚
â”‚    Distributes REPLICATION LOAD                     â”‚
â”‚    â”œâ”€ Each partition uses different replicas       â”‚
â”‚    â”œâ”€ Spreads follower traffic across cluster      â”‚
â”‚    â””â”€ Avoids always hitting same followers         â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Partitioning + CopySet is Powerful:**

Without CopySet (partitioning only):
```
Cluster: 6 nodes, 12 partitions, RF=3

Leadership distribution (good):
  Node 1 leads: Partitions [0, 6]
  Node 2 leads: Partitions [1, 7]
  Node 3 leads: Partitions [2, 8]
  Node 4 leads: Partitions [3, 9]
  Node 5 leads: Partitions [4, 10]
  Node 6 leads: Partitions [5, 11]
  âœ… Write load distributed evenly!

But replica placement (bottleneck):
  All partitions: Replicas=[N1, N2, N3]
  
  âŒ Problem: Nodes 1,2,3 get ALL replication traffic!
  âŒ Nodes 4,5,6 underutilized as followers
  âŒ Nodes 1,2,3 become bottleneck despite distributed leadership
```

With CopySet (partitioning + copyset):
```
Cluster: 6 nodes, 12 partitions, RF=3

Leadership distribution:
  Node 1 leads: Partitions [0, 6]
  Node 2 leads: Partitions [1, 7]
  Node 3 leads: Partitions [2, 8]
  Node 4 leads: Partitions [3, 9]
  Node 5 leads: Partitions [4, 10]
  Node 6 leads: Partitions [5, 11]
  âœ… Write load distributed!

Replica placement (with CopySet):
  Partition 0: Leader=N1, Replicas=[N1, N2, N4]
  Partition 1: Leader=N2, Replicas=[N2, N3, N5]
  Partition 2: Leader=N3, Replicas=[N3, N1, N6]
  Partition 3: Leader=N4, Replicas=[N4, N5, N1]
  Partition 4: Leader=N5, Replicas=[N5, N6, N2]
  Partition 5: Leader=N6, Replicas=[N6, N1, N3]
  Partition 6: Leader=N1, Replicas=[N1, N3, N5]
  Partition 7: Leader=N2, Replicas=[N2, N4, N6]
  Partition 8: Leader=N3, Replicas=[N3, N5, N1]
  Partition 9: Leader=N4, Replicas=[N4, N6, N2]
  Partition 10: Leader=N5, Replicas=[N5, N1, N3]
  Partition 11: Leader=N6, Replicas=[N6, N2, N4]
  
  âœ… Replication load distributed across ALL nodes!
  âœ… No node is overloaded
  âœ… Maximum cluster utilization
```

**The Combined Effect:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Partitioning ONLY                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Write throughput:     âœ… High (distributed)        â”‚
â”‚  Replication capacity: âŒ Limited (same followers)  â”‚
â”‚  Cluster utilization:  âš ï¸  50-70% (uneven)         â”‚
â”‚                                                     â”‚
â”‚  Bottleneck: Follower nodes overwhelmed             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Partitioning + CopySet                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Write throughput:     âœ… High (distributed)        â”‚
â”‚  Replication capacity: âœ… High (distributed)        â”‚
â”‚  Cluster utilization:  âœ… 90%+ (even)               â”‚
â”‚                                                     â”‚
â”‚  No bottleneck: All nodes participate equally       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Load Distribution Comparison:**

```
Traditional (no CopySet):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Node 1: Leader for 2 partitions + Follower for 12 = OVERLOADED âš ï¸
Node 2: Leader for 2 partitions + Follower for 12 = OVERLOADED âš ï¸
Node 3: Leader for 2 partitions + Follower for 12 = OVERLOADED âš ï¸
Node 4: Leader for 2 partitions + Follower for 0  = UNDERUTILIZED âš ï¸
Node 5: Leader for 2 partitions + Follower for 0  = UNDERUTILIZED âš ï¸
Node 6: Leader for 2 partitions + Follower for 0  = UNDERUTILIZED âš ï¸

With CopySet:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Node 1: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
Node 2: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
Node 3: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
Node 4: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
Node 5: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
Node 6: Leader for 2 partitions + Follower for 6  = BALANCED âœ…
```

**Real-World Impact:**

```
Scenario: 10 nodes, 100 partitions, RF=3, 1M writes/sec total

Without CopySet:
  Leaders: Evenly distributed (âœ… 100K writes/node)
  Followers: Replicas always [N1, N2, N3]
    âŒ N1, N2, N3 each handle 3M writes/sec (3x load!)
    âŒ N4-N10 handle 0 replication (wasted capacity)
  
  Result: Cluster CANNOT sustain 1M writes/sec
          Nodes 1-3 are bottleneck

With CopySet:
  Leaders: Evenly distributed (âœ… 100K writes/node)
  Followers: Distributed via CopySet
    âœ… Each node handles ~300K writes/sec as follower
    âœ… All nodes utilized evenly
  
  Result: Cluster EASILY sustains 1M writes/sec
          Can scale to 3M+ writes/sec
```

**Key Insight:**

Partitioning and CopySet are **complementary by design**:

1. **Partitioning** distributes the decision-making (leadership)
2. **CopySet** distributes the work (replication)
3. Together they eliminate **all major bottlenecks**

Without CopySet, partitioning only solves half the problem. With CopySet, you get **true horizontal scalability** where every node contributes equally to both leadership and replication.

This is why **LogDevice invented CopySet replication** - to complement partitioning and achieve maximum cluster utilization! ğŸš€

### Throughput Scaling Examples

#### Example 1: Small Cluster (3 nodes)

```
Configuration:
  Nodes: 3
  Partitions: 9
  Replication Factor: 3
  Write Quorum: 2

Leadership Distribution:
  Node 1 leads: 3 partitions
  Node 2 leads: 3 partitions
  Node 3 leads: 3 partitions

Write Throughput:
  Per-partition: 100K records/sec
  Total: 9 Ã— 100K = 900K records/sec

Read Throughput (from any replica):
  Total: 900K Ã— 3 = 2.7M records/sec
```

#### Example 2: Large Cluster (10 nodes)

```
Configuration:
  Nodes: 10
  Partitions: 100
  Replication Factor: 3
  Write Quorum: 2

Leadership Distribution:
  Each node leads: ~10 partitions
  Load balanced evenly across cluster

Write Throughput:
  Per-partition: 100K records/sec
  Total: 100 Ã— 100K = 10M records/sec

Read Throughput (from any replica):
  Total: 10M Ã— 3 = 30M records/sec
```

### The Fundamental Trade-off

Pyralog's leader-based architecture is a deliberate choice:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LEADER-BASED (Pyralog, Kafka)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Advantages:                                        â”‚
â”‚    âœ… Strong consistency per partition              â”‚
â”‚    âœ… Total ordering within partition               â”‚
â”‚    âœ… Simple programming model                      â”‚
â”‚    âœ… Exactly-once semantics possible               â”‚
â”‚    âœ… No write conflicts                            â”‚
â”‚                                                     â”‚
â”‚  Disadvantages:                                     â”‚
â”‚    âŒ Leader bottleneck per partition               â”‚
â”‚    âŒ Write latency includes network RTT            â”‚
â”‚    âŒ Single point of failure (until failover)      â”‚
â”‚                                                     â”‚
â”‚  Scales via: Many partitions with distributed       â”‚
â”‚              leadership across nodes                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LEADERLESS (Cassandra, Riak)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Advantages:                                        â”‚
â”‚    âœ… No single bottleneck                          â”‚
â”‚    âœ… Write to any node                             â”‚
â”‚    âœ… Better availability                           â”‚
â”‚    âœ… Simpler failure handling                      â”‚
â”‚                                                     â”‚
â”‚  Disadvantages:                                     â”‚
â”‚    âŒ Eventual consistency only                     â”‚
â”‚    âŒ Complex conflict resolution                   â”‚
â”‚    âŒ No total ordering                             â”‚
â”‚    âŒ Read-repair overhead                          â”‚
â”‚                                                     â”‚
â”‚  Scales via: All nodes equal, hash-based routing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pyralog chooses leader-based because:**
1. Distributed logs require ordering (fundamental requirement)
2. Strong consistency simplifies application logic
3. Kafka compatibility demands leader-based model
4. Scales well via partitioning in practice

### Remaining Bottleneck: Hot Partitions

Even with distributed leadership, a single hot partition can become a bottleneck:

```
Problem: Hot Key
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
All records with key="popular-user-id"
  â†’ Same partition (hash-based routing)
  â†’ Same leader
  â†’ Bottleneck on that leader

Example:
  1M requests/sec for one user
  â†’ All to Partition 7
  â†’ Node 2 (leader) overloaded
  â†’ Other nodes underutilized
```

**Mitigations:**

```rust
// 1. Application-level sharding
let partition = if is_hot_key(key) {
    hash(key + random_suffix) % partition_count
} else {
    hash(key) % partition_count
};

// 2. Dynamic partition splitting (see DYNAMIC_PARTITIONS.md)
if partition_load > threshold {
    split_partition(partition_id);
    // Partition 7 â†’ Partitions 7a, 7b
    // Automatic with dynamic partitions!
}

// 3. Read from replicas for hot reads
config.read_policy = ReadPolicy::AnyReplica;
// Spreads read load across 3 nodes
```

**Solution: Dynamic Partitions**

Pyralog supports **dynamic partition splitting and merging** (similar to TiKV's regions):

```
Static Partitions (original):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
10 partitions, fixed at creation
  â†’ Partition 7 gets hot
  â†’ Cannot split without reconfiguration
  â†’ Must over-provision partitions upfront

Dynamic Partitions (NEW):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Start with 5 partitions
  â†’ Partition 3 gets hot (100K writes/sec)
  â†’ Automatic split: P3 â†’ P3a + P3b
  â†’ Each gets 50K writes/sec âœ…
  â†’ No manual intervention!

See DYNAMIC_PARTITIONS.md for complete details.
```

**Benefits of dynamic partitions:**
- âœ… Automatic load balancing
- âœ… Start small, scale as needed
- âœ… Hot partition auto-splitting
- âœ… Cold partition auto-merging
- âœ… True elastic scalability

**Configuration:**

```toml
[log.my_events]
partitioning_mode = "dynamic"
initial_partitions = 5

[log.my_events.split_policy]
max_partition_size = 10_000_000_000  # 10GB
max_write_rate = 100_000.0            # 100K/sec
load_imbalance_threshold = 2.0

[log.my_events.merge_policy]
min_partition_size = 1_000_000_000   # 1GB
min_write_rate = 100.0                # 100/sec
```

### Horizontal Scaling

Adding nodes increases capacity linearly:

```
Add Node Process:
1. Join cluster (Raft membership change)
2. Receive partition assignments
3. Fetch data from existing replicas
4. Join ISR when caught up
5. Start serving as leader for assigned partitions
6. Start serving as follower for other partitions

Result:
  N nodes â†’ N+1 nodes
  Leadership distributed across N+1 nodes
  Throughput increases proportionally
```

**Example scaling timeline:**

```
Initial: 3 nodes, 30 partitions
  Each node: 10 partitions (leader)
  Write capacity: 3M records/sec

Add Node 4:
  Rebalance: Each node now leads ~7-8 partitions
  Write capacity: ~3M records/sec (same, but more headroom)

Add more partitions: 3 nodes, 60 partitions
  Each node: 20 partitions (leader)
  Write capacity: 6M records/sec âœ…

Add Node 5, 6: 6 nodes, 60 partitions
  Each node: 10 partitions (leader)
  Write capacity: 6M records/sec (more fault tolerance)
```

### Partition Rebalancing

Automatic load balancing when cluster topology changes:

```
Rebalance Triggers:
  - New node added
  - Node removed
  - Uneven load distribution
  - Manual rebalancing requested

Rebalance Process:
  1. Calculate optimal partition assignment
     (minimize movement, balance load)
  
  2. Create new replicas on target nodes
     (fetch data from existing replicas)
  
  3. Wait for new replicas to sync
     (join ISR when caught up)
  
  4. Update metadata
     (new leader/follower assignments)
  
  5. Remove old replicas
     (cleanup previous assignments)

During rebalancing:
  âœ… System remains available
  âœ… No data loss
  âœ… Minimal performance impact
```

### Future Optimizations

**1. Partition Splitting**

```rust
// Automatic partition splitting for hot partitions
if partition_metrics.throughput > threshold {
    // Split partition: hash range 0-65535 â†’ 0-32767 + 32768-65535
    split_partition(partition_id)?;
}
```

**2. Dynamic Leader Rebalancing**

```rust
// Move leadership to less-loaded nodes
if node_load_imbalance > threshold {
    rebalance_leaders()?;
    // Transfer leadership without moving data
}
```

**3. Multi-Leader for Geo-Replication**

```rust
// Each datacenter has a leader (eventual consistency)
config.topology = Topology::MultiDatacenter {
    allow_multi_leader: true,
    conflict_resolution: ConflictResolution::LastWriteWins,
};
```

**4. Read Replicas**

```rust
// Dedicated read-only replicas (don't participate in quorum)
config.replication.read_replicas = 2;
// Increases read capacity without affecting write quorum
```

### Scalability Summary

| Aspect | Strategy | Result |
|--------|----------|--------|
| Write throughput | Partitioning | Linear scaling with partitions |
| Read throughput | Replicas | Linear scaling with RF |
| Storage capacity | Add nodes | Linear scaling with nodes |
| Fault tolerance | Replication | Tolerates RF-W node failures |
| Hot partitions | App sharding, future split | Mitigated |
| Leadership | Distributed via partitions | No single bottleneck |

**Real-world capacity example:**
```
10 nodes Ã— 10 partitions/node Ã— 100K records/sec = 10M records/sec
With RF=3: 30M reads/sec possible
```

## Monitoring and Observability

Key metrics:
- Write latency (p50, p99, p999)
- Read latency (p50, p99, p999)
- Throughput (bytes/sec, records/sec)
- Replication lag
- ISR count
- Disk usage
- Network I/O

## Conclusion

Pyralog's architecture represents a synthesis of the best ideas from modern distributed log systems, designed for extreme performance and scalability.

### Key Architectural Innovations

**1. Dual Raft Clusters** â­

Separate consensus domains for massive scalability:
- **Global Raft**: Cluster-wide metadata (membership, partition creation)
- **Per-Partition Raft**: Partition-specific operations (epoch changes, failover)
- **Parallel failover**: 1000 partitions fail over in 10ms (not 10 seconds!)
- **No global bottleneck**: Partition operations don't contend with each other
- **Efficient multiplexing**: 600+ Raft groups per node with batched heartbeats

**2. Epochs (from LogDevice)**

The most impactful optimization:
- **100x throughput improvement** by decoupling offset assignment from consensus
- Per-partition Raft consensus once per epoch (not per record!)
- Local offset increment: millions of records/sec without consensus bottleneck
- Safe failover without split-brain scenarios

**3. Smart Client Pattern (from Kafka)**

Eliminates proxy overhead:
- Direct connection to partition leaders (1 hop vs 2)
- Client-side load balancing via metadata caching
- Metadata refresh only on topology changes (~5 min)
- Amortized overhead: essentially zero

**4. Distributed Leadership via Partitioning**

Spreads write decisions across the cluster:
- Each partition has one leader
- Leadership distributed across all nodes
- Linear scaling: N partitions â†’ NÃ— write throughput
- No single leader bottleneck

**5. CopySet Replication (from LogDevice)**

Critical complement to partitioning:
- Two strategies: Per-partition (simple) or per-record (maximum distribution)
- Per-record: Distributes replication load across entire cluster
- Per-record with coordinator mode: Leader doesn't store data (99%+ less I/O!)
- Achieves 90%+ cluster utilization vs 50% without it
- Every node contributes equally to leadership and replication
- Leader can handle 20x-50x more partitions in coordinator mode

**6. Flexible Quorums**

Runtime configurability:
- Configure CAP position per use case
- Strong consistency (CP), high availability (AP), or balanced
- W+R > RF constraint ensures safety
- No architectural lock-in

**7. Multiple Optimizations Working Together**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         The Synergistic Effect                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Dual Raft + Partitioning                                â”‚
â”‚    = Parallel failover, no global bottleneck            â”‚
â”‚    â†’ 1000 partitions fail over simultaneously            â”‚
â”‚                                                          â”‚
â”‚  Epochs + Per-Partition Raft                             â”‚
â”‚    = Fast epoch changes (only partition replicas vote)  â”‚
â”‚    â†’ 10ms failover instead of seconds                   â”‚
â”‚                                                          â”‚
â”‚  Epochs + Smart Clients                                  â”‚
â”‚    = Million writes/sec with sub-ms latency             â”‚
â”‚                                                          â”‚
â”‚  Partitioning + CopySet                                  â”‚
â”‚    = True horizontal scalability, no bottlenecks        â”‚
â”‚                                                          â”‚
â”‚  Flexible Quorums + ISR                                  â”‚
â”‚    = Configurable consistency/availability              â”‚
â”‚                                                          â”‚
â”‚  Global Raft + Per-Partition Raft                        â”‚
â”‚    = Strong consistency without throughput penalty      â”‚
â”‚    â†’ Cluster ops separate from partition ops            â”‚
â”‚                                                          â”‚
â”‚  Write Cache + Zero-Copy                                 â”‚
â”‚    = Sub-millisecond latencies                          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Makes This Architecture Special

**Complementary Design:**

Every component enhances the others:
- Dual Raft separates concerns â†’ enables parallel partition operations
- Epochs remove consensus bottleneck â†’ enables million writes/sec  
- Per-partition Raft makes epochs fast â†’ 10ms epoch changes (not seconds)
- Smart clients avoid proxy â†’ enables direct scaling
- Partitioning distributes leadership â†’ enables horizontal scaling
- CopySet distributes replication â†’ prevents follower bottleneck
- Together: **No single point of bottleneck anywhere!**

**Production-Ready Capabilities:**

```
Expected Performance (10 nodes, 100 partitions):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write throughput:  10M+ records/sec
Read throughput:   30M+ records/sec (RF=3)
Write latency:     < 1ms (p99, with cache)
Read latency:      < 0.5ms (p99, with mmap)
Scalability:       Linear with nodes/partitions
Consistency:       Configurable (CP to AP spectrum)
Availability:      Tolerates RF-W node failures
```

**Learning from the Best:**

Pyralog synthesizes innovations from:
- **LogDevice** (Facebook): Epochs, CopySet, flexible quorums, hierarchical storage
- **Kafka** (LinkedIn): Smart clients, partitioning, ISR, log-structured storage
- **Redpanda** (Vectorized): Write caching, zero-copy I/O, thread-per-core
- **Raft** (Stanford): Proven consensus algorithm for cluster coordination

### Architectural Philosophy

**1. Optimize the Hot Path**

- Write path: Epochs avoid Raft, cache avoids fsync, smart client avoids proxy
- Read path: mmap for zero-copy, ISR for flexibility, metadata for direct routing
- Result: Sub-millisecond latencies at million ops/sec

**2. Eliminate Bottlenecks at Every Level**

- Global consensus for everything â†’ Dual Raft (separate domains)
- Single leader â†’ Distributed leadership (partitioning)
- Follower overload â†’ Distributed replication (CopySet)
- Proxy overhead â†’ Smart clients (direct routing)
- Consensus per record â†’ Consensus per epoch (100x gain)
- Sequential partition failover â†’ Parallel per-partition Raft (1000x faster)

**3. Make Trade-offs Configurable**

- CAP spectrum: Choose consistency vs availability at runtime
- Read policy: Leader, replicas, quorum, or nearest
- Quorum sizes: Balance durability vs latency
- No one-size-fits-all: You decide the trade-offs

**4. Horizontal Scalability**

- Add nodes â†’ Add capacity (linear scaling)
- Add partitions â†’ Add throughput (linear scaling)
- Replication â†’ Fault tolerance (configurable)
- Result: Start small, scale to billions of records/day

### The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Why Pyralog's Architecture Succeeds                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Traditional Distributed Log:                       â”‚
â”‚    âŒ Leader bottleneck                             â”‚
â”‚    âŒ Consensus per record                          â”‚
â”‚    âŒ Proxy overhead                                â”‚
â”‚    âŒ Follower bottleneck                           â”‚
â”‚    âŒ Fixed consistency model                       â”‚
â”‚                                                     â”‚
â”‚  Pyralog's Solution:                                   â”‚
â”‚    âœ… Distributed leadership (partitioning)         â”‚
â”‚    âœ… Consensus per epoch (100x faster)             â”‚
â”‚    âœ… Smart clients (direct routing)                â”‚
â”‚    âœ… Distributed replication (CopySet)             â”‚
â”‚    âœ… Flexible quorums (configurable)               â”‚
â”‚                                                     â”‚
â”‚  Result: 10M+ writes/sec, sub-ms latency,          â”‚
â”‚          horizontal scaling, no bottlenecks         â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Final Thoughts

Pyralog isn't just another distributed log - it's a **synthesis of proven innovations** that complement each other perfectly:

1. **Epochs** make high throughput possible (remove consensus bottleneck)
2. **Smart clients** make it scalable (remove proxy bottleneck)
3. **Partitioning** makes it distributed (remove leader bottleneck)
4. **CopySet** makes it efficient (remove follower bottleneck)
5. **Flexible quorums** make it adaptable (configure for your needs)

Each innovation solves a specific bottleneck. Together, they create a system with **no fundamental limitations** - just add more nodes and partitions to scale.

**This is the power of learning from a decade of production distributed log systems and combining their best ideas in a modern, Rust-based implementation.**

The modular design allows for easy extension and customization while maintaining strong guarantees about data durability and consistency. Whether you need strong consistency for financial transactions or high availability for analytics, Pyralog's architecture can be configured to meet your requirements.

**Welcome to the next generation of distributed logs.** ğŸš€

