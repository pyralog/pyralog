# Pyralog vs TiKV: Architectural Comparison

A detailed comparison between Pyralog (distributed log) and TiKV (distributed key-value store).

## Table of Contents

1. [Overview](#overview)
2. [Architecture Similarities](#architecture-similarities)
3. [Key Differences](#key-differences)
4. [Multi-Raft Comparison](#multi-raft-comparison)
5. [Use Cases](#use-cases)
6. [Performance Characteristics](#performance-characteristics)
7. [Design Philosophy](#design-philosophy)
8. [When to Use Which](#when-to-use-which)

---

## Overview

### Pyralog

**Type**: Distributed log / Streaming platform  
**Language**: Rust  
**Inspired by**: LogDevice, Kafka, Redpanda  
**Primary Use**: Event streaming, message queues, change data capture  

### TiKV

**Type**: Distributed transactional key-value store  
**Language**: Rust  
**Part of**: TiDB (distributed SQL database)  
**Primary Use**: Database storage engine, distributed transactions  

---

## Architecture Similarities

Both systems share fundamental distributed systems patterns:

### 1. Multi-Raft Architecture âœ…

**Both use dual Raft clusters!**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pyralog Multi-Raft                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Global Raft: [N1, N2, N3, N4, N5]                     â”‚
â”‚    â†’ Cluster membership, partition metadata            â”‚
â”‚                                                         â”‚
â”‚  Per-Partition Raft:                                    â”‚
â”‚    Partition 0: [N1, N2, N3]                           â”‚
â”‚    Partition 1: [N2, N3, N4]                           â”‚
â”‚    â†’ Epoch changes, partition leadership               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TiKV Multi-Raft                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  PD (Placement Driver): Centralized metadata           â”‚
â”‚    â†’ Uses Raft for its own HA                          â”‚
â”‚    â†’ Manages cluster metadata, region assignments      â”‚
â”‚                                                         â”‚
â”‚  Per-Region Raft:                                       â”‚
â”‚    Region 0: [N1, N2, N3]                              â”‚
â”‚    Region 1: [N2, N3, N4]                              â”‚
â”‚    â†’ Data replication, leadership for key ranges       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: Both avoid global Raft bottleneck by using per-shard Raft!

### 2. Sharding Strategy

**Pyralog: Partitions**
```rust
// Partition by key hash
partition_id = hash(record.key) % partition_count

// Or by explicit partition
client.produce_to_partition(log_id, partition_id, record)
```

**TiKV: Regions (Dynamic)**
```rust
// Region = contiguous key range
Region 1: [key_a, key_m)
Region 2: [key_m, key_z)

// Regions split automatically when they grow too large
if region.size > threshold {
    split_region(region);
}
```

**Similarity**: Both distribute data across independent shards with separate Raft groups.

**Difference**: TiKV's regions split/merge dynamically; Pyralog's partitions are static (pre-allocated).

### 3. Rust Implementation

Both are implemented in Rust for:
- âœ… Memory safety without GC pauses
- âœ… Zero-cost abstractions
- âœ… Fearless concurrency
- âœ… High performance

### 4. Flexible Replication

**Pyralog:**
```toml
[replication]
replication_factor = 3
write_quorum = 2
read_quorum = 2
```

**TiKV:**
```
Region replicas: 3 (default)
Raft quorum: Majority (2/3)
```

Both use quorum-based replication for fault tolerance.

---

## Key Differences

### 1. Data Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pyralog: Append-Only Log                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Operations:                                            â”‚
â”‚    âœ“ Append (write to end)                             â”‚
â”‚    âœ“ Read by offset/time                               â”‚
â”‚    âœ“ Scan (sequential reads)                           â”‚
â”‚    âœ— Update (not supported)                            â”‚
â”‚    âœ— Delete (only via compaction)                      â”‚
â”‚                                                         â”‚
â”‚  Storage:                                               â”‚
â”‚    Offset 0: Record A                                  â”‚
â”‚    Offset 1: Record B                                  â”‚
â”‚    Offset 2: Record C                                  â”‚
â”‚    â†’ Immutable, sequential                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TiKV: Key-Value Store (LSM Tree)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Operations:                                            â”‚
â”‚    âœ“ Get (read by key)                                 â”‚
â”‚    âœ“ Put (insert/update)                               â”‚
â”‚    âœ“ Delete                                            â”‚
â”‚    âœ“ Scan (range queries)                              â”‚
â”‚    âœ“ Transactions (ACID)                               â”‚
â”‚                                                         â”‚
â”‚  Storage (RocksDB):                                     â”‚
â”‚    key_a â†’ value_1                                     â”‚
â”‚    key_b â†’ value_2                                     â”‚
â”‚    key_c â†’ value_3                                     â”‚
â”‚    â†’ Mutable, random access                            â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Transaction Support

**Pyralog:**
- âŒ No built-in ACID transactions (by design)
- âœ… Atomic appends within a partition
- âœ… Exactly-once semantics (idempotent producers)
- âœ… Cross-partition atomicity (future: 2PC for multi-partition writes)

**TiKV:**
- âœ… Full ACID transactions (Percolator model)
- âœ… Distributed transactions across regions
- âœ… Snapshot isolation
- âœ… Optimistic/Pessimistic locking

**Use case difference:**
- Pyralog: Event streaming, where order matters more than updates
- TiKV: Database storage, where updates and transactions are essential

### 3. Consistency Model

**Pyralog:**
```
Per-Partition Sequential Consistency:
  - Records within a partition are totally ordered
  - Across partitions: no ordering guarantees
  - Reads can be stale (configurable with quorums)

Epoch-based consistency:
  - Epoch = generation of partition leader
  - All records in epoch are ordered
  - Epoch change = failover boundary
```

**TiKV:**
```
Linearizable Reads/Writes (optional):
  - Read Index from leader
  - Ensures reads see latest committed writes
  
Snapshot Isolation:
  - MVCC timestamps
  - Transactions see consistent snapshot
  - Causality preserved across regions
```

### 4. Storage Engine

**Pyralog:**
```rust
// Log-structured, append-only
pub struct LogStorage {
    segments: Vec<Segment>,     // Sequential files
    active_segment: Segment,    // Current write target
    index: SparseIndex,         // Offset â†’ file position
}

// Optimized for:
âœ“ Sequential writes (1M+ writes/sec)
âœ“ Sequential reads (scanning)
âœ“ Time-series data
âœ— Random updates
```

**TiKV:**
```rust
// RocksDB (LSM Tree)
pub struct RocksDBEngine {
    db: Arc<DB>,
    cf_handles: HashMap<String, ColumnFamily>,
}

// Optimized for:
âœ“ Random reads (point queries)
âœ“ Range scans
âœ“ Updates and deletes
âœ“ Compaction (merging SSTables)
```

### 5. Metadata Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pyralog: Distributed Metadata                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Global Raft cluster (embedded):                        â”‚
â”‚    - All nodes participate                             â”‚
â”‚    - Metadata replicated to all nodes                  â”‚
â”‚    - No separate metadata service                      â”‚
â”‚                                                         â”‚
â”‚  Advantages:                                            â”‚
â”‚    âœ“ Simple deployment (single binary)                 â”‚
â”‚    âœ“ No external dependencies                          â”‚
â”‚    âœ— Metadata on all nodes (memory overhead)           â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TiKV: Centralized Metadata (PD)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Placement Driver (separate service):                   â”‚
â”‚    - Dedicated metadata cluster (3-5 nodes)            â”‚
â”‚    - Global view of cluster state                      â”‚
â”‚    - Schedules region moves, splits, merges            â”‚
â”‚                                                         â”‚
â”‚  Advantages:                                            â”‚
â”‚    âœ“ Global optimization (load balancing)              â”‚
â”‚    âœ“ Advanced scheduling policies                      â”‚
â”‚    âœ— Additional component to manage                    â”‚
â”‚    âœ— PD becomes critical path                          â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Client Protocol

**Pyralog: Smart Clients (Kafka-style)**
```rust
// Client fetches metadata, routes directly
let metadata = client.fetch_metadata(log_id).await?;
let leader = metadata.get_leader(partition_id);
client.send_to_leader(leader, record).await?;

// Benefits:
âœ“ No proxy overhead
âœ“ Direct routing (1 hop)
âœ“ Client-side load balancing
```

**TiKV: Region Cache + PD**
```rust
// Client caches region info, queries PD on miss
let region = client.get_region_by_key(key).await?;
if region.leader_changed {
    region = client.query_pd(key).await?;
}
client.send_to_region(region, kv_request).await?;

// Benefits:
âœ“ Dynamic region routing
âœ“ PD handles complexity
âœ— PD query on cache miss
```

---

## Multi-Raft Comparison

### Resource Usage

**Pyralog (5 nodes, 1000 partitions, RF=3):**
```
Per Node:
  Global Raft: 1 group
  Partition Rafts: ~600 groups (60% of partitions)
  Total: ~601 Raft groups

Memory: 601 Ã— 10 KB â‰ˆ 6 MB
Disk: 601 Ã— 1 MB â‰ˆ 600 MB
Network: ~1200 heartbeats/sec
```

**TiKV (5 nodes, 1000 regions, RF=3):**
```
Per Node:
  Region Rafts: ~600 groups (60% of regions)
  PD client: Metadata cache
  Total: ~600 Raft groups

Memory: 600 Ã— 10 KB â‰ˆ 6 MB (similar!)
Disk: 600 Ã— 1 MB â‰ˆ 600 MB (RocksDB WAL)
Network: ~1200 heartbeats/sec + PD reports
```

**Conclusion**: Multi-Raft overhead is similar in both systems!

### Failure Handling

**Pyralog Partition Leader Failure:**
```
1. Per-Partition Raft detects leader failure
   â†’ Partition 0: [N1, N2, N3], N1 fails
   
2. Election among partition replicas
   â†’ N2 or N3 elected (10-300ms)
   
3. New leader activates new epoch
   â†’ Proposes to Partition 0 Raft
   â†’ Committed when majority of [N2, N3] ack
   
4. Clients redirect to new leader
   â†’ Metadata updated via Global Raft
   
Time: ~300ms total
```

**TiKV Region Leader Failure:**
```
1. Per-Region Raft detects leader failure
   â†’ Region 0: [N1, N2, N3], N1 fails
   
2. Election among region replicas
   â†’ N2 or N3 elected (10-300ms)
   
3. New leader reports to PD
   â†’ PD updates region metadata
   â†’ Broadcasts to all TiKV nodes
   
4. Clients fetch new metadata from PD
   â†’ Cache updated
   
Time: ~300ms total (similar!)
```

### Scalability

**Pyralog:**
```
Static partitions:
  - Partition count set at creation
  - Rebalance on node add/remove
  - No automatic splitting
  
Scaling strategy:
  1. Over-provision partitions (100-1000)
  2. Distribute across nodes
  3. Rebalance when adding nodes
  
Best for:
  âœ“ Predictable workloads
  âœ“ Known partition count
  âœ— Highly uneven key distributions
```

**TiKV:**
```
Dynamic regions:
  - Regions split when they grow
  - Regions merge when they shrink
  - PD schedules region moves
  
Scaling strategy:
  1. Start with few regions
  2. Automatic splitting as data grows
  3. PD rebalances automatically
  
Best for:
  âœ“ Unpredictable growth
  âœ“ Uneven key distributions
  âœ“ Automatic rebalancing
```

---

## Use Cases

### Pyralog Excels At

**1. Event Streaming**
```rust
// High-throughput event ingestion
for event in events {
    dlog.append(event).await?;
}

// Sequential consumption
let records = dlog.read_from_offset(offset, 1000).await?;
```

**2. Message Queues**
```rust
// Multiple consumers, each partition
consumer_group.consume(log_id, |record| {
    process(record);
}).await?;
```

**3. Change Data Capture**
```rust
// Database changes â†’ Pyralog â†’ downstream systems
db.on_change(|change| {
    dlog.append(change).await;
});
```

**4. Time-Series Data**
```rust
// Metrics, logs, traces
metrics.on_sample(|sample| {
    dlog.append(sample).await;
});
```

### TiKV Excels At

**1. Database Storage**
```rust
// SQL queries via TiDB
SELECT * FROM users WHERE id = 123;
// â†’ TiKV get(key="users:123")

UPDATE users SET name = 'Alice' WHERE id = 123;
// â†’ TiKV put(key="users:123", value=...)
```

**2. Distributed Transactions**
```rust
// Multi-key transaction
txn.begin();
txn.put("account:1", balance - 100);
txn.put("account:2", balance + 100);
txn.commit()?; // ACID guaranteed
```

**3. Key-Value Workloads**
```rust
// Session storage, caching, etc.
tikv.put("session:abc", session_data).await?;
let session = tikv.get("session:abc").await?;
```

**4. Metadata Storage**
```rust
// Application metadata, configuration
tikv.put("config:feature_flags", flags).await?;
```

---

## Performance Characteristics

### Throughput

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Write Throughput (10 nodes, RF=3)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Pyralog:                                                    â”‚
â”‚    Sequential writes: 10M+ records/sec                   â”‚
â”‚    Per partition: 1M+ records/sec                        â”‚
â”‚    Batch size: 1000 records                              â”‚
â”‚                                                          â”‚
â”‚  TiKV:                                                    â”‚
â”‚    Random writes: 200K+ ops/sec                          â”‚
â”‚    Per region: ~20K ops/sec                              â”‚
â”‚    Transaction overhead: ~2-3ms per txn                  â”‚
â”‚                                                          â”‚
â”‚  Winner: Pyralog (50x higher for sequential writes)         â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Read Throughput (10 nodes, RF=3)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Pyralog:                                                    â”‚
â”‚    Sequential scans: 30M+ records/sec                    â”‚
â”‚    Random reads: 1M+ reads/sec                           â”‚
â”‚    (mmap + zero-copy)                                    â”‚
â”‚                                                          â”‚
â”‚  TiKV:                                                    â”‚
â”‚    Point reads: 500K+ ops/sec                            â”‚
â”‚    Range scans: 100K+ ops/sec                            â”‚
â”‚    (RocksDB block cache)                                 â”‚
â”‚                                                          â”‚
â”‚  Winner: Depends on access pattern                       â”‚
â”‚    - Sequential: Pyralog                                    â”‚
â”‚    - Random: TiKV                                        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Latency

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Write Latency (p99)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Pyralog:                                                    â”‚
â”‚    With write cache: < 1ms                               â”‚
â”‚    Durable (fsync): ~5ms                                 â”‚
â”‚    Replication: ~10ms (quorum)                           â”‚
â”‚                                                          â”‚
â”‚  TiKV:                                                    â”‚
â”‚    Single put: ~10ms                                     â”‚
â”‚    Transaction: ~20ms                                    â”‚
â”‚    (2PC + Raft commit)                                   â”‚
â”‚                                                          â”‚
â”‚  Winner: Pyralog (10x lower latency)                        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Read Latency (p99)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Pyralog:                                                    â”‚
â”‚    Sequential (hot): < 0.5ms (mmap)                      â”‚
â”‚    Random (hot): ~1ms                                    â”‚
â”‚    Cold: ~10ms (disk seek)                               â”‚
â”‚                                                          â”‚
â”‚  TiKV:                                                    â”‚
â”‚    Point read (hot): ~1ms                                â”‚
â”‚    Point read (cold): ~5ms                               â”‚
â”‚    Range scan: ~10ms                                     â”‚
â”‚                                                          â”‚
â”‚  Winner: Similar for hot data, Pyralog for scans            â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scalability

Both scale linearly with nodes:

```
Pyralog:
  10 nodes: 10M writes/sec
  20 nodes: 20M writes/sec
  Limit: Network bandwidth

TiKV:
  10 nodes: 200K writes/sec
  20 nodes: 400K writes/sec
  Limit: Transaction coordination
```

---

## Design Philosophy

### Pyralog: Optimized for Streaming

**Priorities:**
1. â­ Sequential write throughput
2. â­ Low-latency appends
3. â­ Efficient sequential scans
4. âœ… Immutability (simplifies replication)
5. âœ… Simple deployment

**Trade-offs:**
- âŒ No random updates
- âŒ No transactions
- âŒ Static partitioning

### TiKV: Optimized for Databases

**Priorities:**
1. â­ ACID transactions
2. â­ Random access performance
3. â­ Dynamic load balancing
4. âœ… Mutable data
5. âœ… Strong consistency

**Trade-offs:**
- âš ï¸  Transaction overhead (2PC)
- âš ï¸  Complex deployment (PD + TiKV)
- âš ï¸  Lower write throughput

---

## When to Use Which

### Use Pyralog When

âœ… **Event streaming is the primary use case**
```
Examples:
  - Click streams
  - Application logs
  - IoT sensor data
  - CDC from databases
  - Message queues
```

âœ… **Sequential access dominates**
```
Workload:
  - Append at end
  - Read from beginning
  - Process in order
```

âœ… **High throughput > transactions**
```
Requirements:
  - 1M+ events/sec
  - Sub-millisecond latency
  - No need for updates/deletes
```

âœ… **Simpler operations preferred**
```
Deployment:
  - Single binary
  - No external dependencies
  - Kafka-like semantics
```

### Use TiKV When

âœ… **Need a distributed database**
```
Examples:
  - SQL database (via TiDB)
  - Key-value store
  - Distributed cache
  - Metadata storage
```

âœ… **Transactions are essential**
```
Workload:
  - Multi-key updates
  - ACID guarantees
  - Snapshot isolation
```

âœ… **Random access dominates**
```
Access pattern:
  - Point queries by key
  - Updates and deletes
  - Range scans
```

âœ… **Dynamic scaling needed**
```
Requirements:
  - Unpredictable growth
  - Automatic rebalancing
  - Hot key handling
```

---

## Hybrid Approaches

### Can They Work Together?

**YES!** Common patterns:

**Pattern 1: Pyralog â†’ TiKV**
```
Application â†’ Pyralog (event stream)
              â†“
           Consumer â†’ TiKV (materialized views)
```

Example:
```rust
// Write events to Pyralog (high throughput)
dlog.append(Event {
    user_id: 123,
    action: "purchase",
    amount: 99.99,
}).await?;

// Consume and materialize to TiKV
consumer.process(|event| {
    // Update user stats in TiKV
    tikv.transaction(|txn| {
        let stats = txn.get("user_stats:123")?;
        stats.total_spent += event.amount;
        txn.put("user_stats:123", stats)?;
        txn.commit()
    }).await?;
}).await?;
```

**Pattern 2: Database CDC**
```
TiDB â†’ Pyralog (change stream)
        â†“
     Downstream consumers
```

Example:
```rust
// Capture TiDB changes to Pyralog
tidb.on_change(|change| {
    dlog.append(change).await;
});

// Multiple consumers process changes
search_indexer.consume_from(dlog);
analytics.consume_from(dlog);
cache_invalidator.consume_from(dlog);
```

---

## Conclusion

### Similarities

Both Pyralog and TiKV:
- âœ… Use multi-Raft for scalability
- âœ… Written in Rust for performance
- âœ… Support 1000+ shards per node
- âœ… Provide strong consistency
- âœ… Scale linearly with nodes

### Key Differences

| Feature | Pyralog | TiKV |
|---------|------|------|
| **Data Model** | Append-only log | Mutable key-value |
| **Access Pattern** | Sequential | Random |
| **Transactions** | No (by design) | Yes (ACID) |
| **Write Throughput** | 10M+ ops/sec | 200K+ ops/sec |
| **Latency** | < 1ms | ~10ms |
| **Deployment** | Single binary | Multi-component (PD + TiKV) |
| **Use Case** | Streaming, logs | Database, storage |

### The Big Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage Spectrum                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Append-Only â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Mutable          â”‚
â”‚                                                         â”‚
â”‚  Pyralog            Kafka          TiKV        PostgreSQL â”‚
â”‚  â†“                â†“              â†“              â†“       â”‚
â”‚  Pure log    Event stream   KV store    Relational DB â”‚
â”‚                                                         â”‚
â”‚  High throughput â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Rich features   â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Bottom Line:**
- **Pyralog**: Best-in-class distributed log for streaming workloads
- **TiKV**: Best-in-class distributed KV store for database workloads
- **Together**: Complementary, can be used in the same architecture!

Both systems prove that **multi-Raft is the key to scaling distributed systems** beyond single-cluster limitations! ğŸš€

