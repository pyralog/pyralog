# Introducing Pyralog: Rethinking Distributed Logs

**A unified distributed platform that achieves 10M+ writes/sec and eliminates coordination bottlenecks through novel architectural primitives**

*Published: November 1, 2025 ‚Ä¢ Reading time: 12 min*

---

## The Fragmentation Problem

Modern distributed applications require **five separate systems**:

| System | Purpose | Cost |
|--------|---------|------|
| **Kafka** | Event streaming | Complex ops, Java JVM |
| **PostgreSQL/TiKV** | Transactions | Limited throughput |
| **Flink/Spark** | Stream processing | Separate compute layer |
| **ClickHouse/Snowflake** | Analytics | Data duplication |
| **Jaeger/Elasticsearch** | Observability | High latency |

### The Real Cost

At every integration point, you pay:
- ‚ùå **Network overhead** (serialize ‚Üí transmit ‚Üí deserialize)
- ‚ùå **Data duplication** (same data in multiple systems)
- ‚ùå **Consistency gaps** (different semantics everywhere)
- ‚ùå **Operational complexity** (5+ systems to deploy, monitor, upgrade)
- ‚ùå **Infrastructure costs** (separate clusters for each)

```
Traditional Stack:

Application
    ‚Üì (ETL)
  Kafka (streaming)
    ‚Üì (ETL)
  Flink (processing)
    ‚Üì (ETL)
  ClickHouse (analytics)
    ‚Üì
  Grafana (dashboards)
  
Result: 500ms+ end-to-end latency
```

---

## Pyralog: Unified Platform

**One system. All capabilities. Better performance.**

```
Pyralog Platform:

Application
    ‚Üì (Arrow IPC)
  Pyralog
    ‚Ä¢ Streaming ‚úÖ
    ‚Ä¢ Processing ‚úÖ
    ‚Ä¢ Analytics ‚úÖ
    ‚Ä¢ Transactions ‚úÖ
    ‚Ä¢ Observability ‚úÖ
    
Result: <5ms end-to-end latency (100√ó faster!)
```

### Performance Targets

| Metric | Traditional | Pyralog | Improvement |
|--------|-------------|---------|-------------|
| **Writes/sec** | 3.2M (Kafka) | 10M+ | **3.1√ó** |
| **Transactions** | 500 tx/s (TiKV) | 4M+ tx/s | **8,000√ó** |
| **Analytics** | Separate system | Built-in SQL | **Zero ETL** |
| **Latency** | 10-100ms | <1ms | **10-100√ó** |
| **Coordinator ops** | 500K/s | 28B+/s | **56,000√ó** |

---

## Three Core Innovations

### 1. üóø Obelisk Sequencer: Persistent Atomic Counters

**The Problem:**
- Standard `AtomicU64`: Fast but lost on crash ‚ùå
- Database sequences: Durable but slow ‚ùå

**The Solution:**
```rust
// Obelisk Sequencer: Fast AND durable ‚úÖ
pub struct ObeliskSequencer {
    counter: AtomicU64,
    file: File,  // File size = counter value!
}

impl ObeliskSequencer {
    pub fn next(&self) -> u64 {
        let value = self.counter.fetch_add(1, Ordering::SeqCst);
        self.file.write_all(&[0])?;  // Append 1 byte
        value
    }
    
    pub fn recover(path: &Path) -> u64 {
        fs::metadata(path)?.len()  // Read file size!
    }
}
```

**Key Innovation:** Use file size as the counter value!
- Write: ~1-2 ¬µs (with fsync batching)
- Recovery: ~2 ¬µs (just `stat()` syscall)
- Disk: ~8 KB (sparse file for billions!)
- **Crash-safe by design**

### 2. ‚òÄÔ∏è Pharaoh Network: Coordination Without Bottlenecks

**Traditional Systems:**
```
Centralized Coordinator:
  1 TSO ‚Üí 500K timestamps/sec ‚Üí BOTTLENECK! ‚ùå

Kafka:
  1 Zookeeper ‚Üí metadata bottleneck ‚ùå
  
TiKV:
  1 TSO ‚Üí all transactions wait ‚ùå
```

**Pyralog's Pharaoh Network:**
```
Distributed Coordinators:
  1,024 TSO nodes ‚Üí 4+ billion timestamps/sec ‚úÖ
  1,024 Tx Coordinators ‚Üí 4+ billion tx/sec ‚úÖ
  1,024 Session Managers ‚Üí 4+ billion sessions/sec ‚úÖ
  
  Total: 28+ billion operations/sec
  No centralized bottlenecks!
```

**How it works:**
1. Client hashes request ID
2. Routes to one of 1,024 coordinators
3. Each coordinator uses Obelisk Sequencer (stateless!)
4. Linear scaling: 2,048 nodes = 2√ó throughput

### 3. Apache Arrow Native Storage

**Traditional Row-Based:**
```
Record { user_id: 123, value: 456, timestamp: ... }
Record { user_id: 789, value: 101, timestamp: ... }
...

Query: SUM(value)
‚ùå Must deserialize entire records
‚ùå No SIMD vectorization
‚ùå Cache-inefficient
```

**Pyralog's Arrow Columnar:**
```
user_id:    [123, 789, ...]
value:      [456, 101, ...]  ‚Üê Query only this column!
timestamp:  [...]

Query: SUM(value)
‚úÖ Read only needed columns (zero-copy)
‚úÖ SIMD vectorization (process 8-16 values at once)
‚úÖ Cache-efficient (sequential access)
‚úÖ 10-100√ó faster analytics
```

**Integration:**
- **DataFusion** for SQL queries
- **Polars** for DataFrame operations
- **Arrow Flight** for zero-copy RPC
- **Native Python/Rust/Java** bindings

---

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Pyralog Platform                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  Client Layer:                                             ‚îÇ
‚îÇ    SQL ‚Ä¢ Batuta ‚Ä¢ PRQL ‚Ä¢ GraphQL ‚Ä¢ JSON-RPC/WS            ‚îÇ
‚îÇ                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  ‚òÄÔ∏è Pharaoh Network (Obelisk Nodes):                      ‚îÇ
‚îÇ    ‚Ä¢ 1,024 TSO nodes        ‚Üí 4B timestamps/sec           ‚îÇ
‚îÇ    ‚Ä¢ 1,024 Tx Coordinators  ‚Üí 4B transactions/sec         ‚îÇ
‚îÇ    ‚Ä¢ 1,024 Session Managers ‚Üí 4B sessions/sec             ‚îÇ
‚îÇ    ‚Ä¢ 1,024 Consumer Coords  ‚Üí 4B ops/sec                  ‚îÇ
‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ
‚îÇ    Total: 28+ billion ops/sec                             ‚îÇ
‚îÇ    (Lightweight, stateless, coordination-free)            ‚îÇ
‚îÇ                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  üî∫ Pyramid Nodes (Storage/Consensus/Compute):            ‚îÇ
‚îÇ    ‚Ä¢ Per-partition Raft consensus                         ‚îÇ
‚îÇ    ‚Ä¢ LSM-Tree storage + File references                   ‚îÇ
‚îÇ    ‚Ä¢ Multi-model support (6 data models)                  ‚îÇ
‚îÇ    ‚Ä¢ DataFusion SQL engine                                ‚îÇ
‚îÇ    ‚Ä¢ Actor-based distributed queries                      ‚îÇ
‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÇ
‚îÇ    10M+ writes/sec cluster-wide                           ‚îÇ
‚îÇ    (Heavier, stateful, Raft-coordinated)                  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  Storage Layer:                                            ‚îÇ
‚îÇ    ‚Ä¢ Apache Arrow (columnar, zero-copy)                   ‚îÇ
‚îÇ    ‚Ä¢ Parquet segments (persistent)                        ‚îÇ
‚îÇ    ‚Ä¢ Memory-mapped files (cold data)                      ‚îÇ
‚îÇ    ‚Ä¢ BLAKE3 Merkle trees (verification)                   ‚îÇ
‚îÇ                                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## What This Enables

### 1. Unified Streaming + Analytics

```rust
// Write events
client.produce("user-events", event).await?;

// Query them immediately with SQL (zero ETL!)
let results = client.sql("
    SELECT 
        user_id,
        COUNT(*) as events,
        AVG(session_duration) as avg_duration
    FROM user_events
    WHERE timestamp > now() - INTERVAL '5 minutes'
    GROUP BY user_id
    HAVING COUNT(*) > 100
").await?;
```

**No separate analytics system. No delays. No ETL.**

### 2. Exactly-Once Stream Processing

```rust
// Built-in exactly-once semantics
let processed = stream
    .filter(|event| event.value > 100)
    .aggregate(
        window::tumbling(Duration::from_secs(60)),
        |acc, event| acc + event.value
    )
    .with_exactly_once()  // ‚Üê Native!
    .write_to("aggregated-events")
    .await?;
```

**Kafka requires complex configuration. Pyralog makes it native.**

### 3. Distributed ACID Transactions

```rust
// 4M+ transactions per second
let tx = client.begin_transaction().await?;

tx.write("inventory", decrease_stock(product_id, qty)).await?;
tx.write("orders", create_order(user_id, product_id)).await?;
tx.write("payments", deduct_balance(user_id, price)).await?;

tx.commit().await?;  // All-or-nothing
```

**8,000√ó faster than TiKV's centralized TSO.**

### 4. Real-Time Observability

```rust
// Ingest traces (OpenTelemetry)
pyralog.ingest_otlp(trace).await?;

// Query with SQL (10-50√ó faster than Elasticsearch)
let slow = client.sql("
    SELECT service_name, span_name, duration_ms, trace_id
    FROM traces
    WHERE duration_ms > 1000
      AND timestamp > now() - INTERVAL '1 hour'
    ORDER BY duration_ms DESC
    LIMIT 100
").await?;
```

**10-50√ó faster writes. 5-10√ó faster queries. Zero ETL.**

### 5. Multi-Model Database

```rust
// Relational (SQL)
client.sql("SELECT * FROM users WHERE age > 25").await?;

// Document (JSON)
client.json_query("users", json!({"age": {"$gt": 25}})).await?;

// Graph (Cypher)
client.cypher("MATCH (u:User)-[:FOLLOWS]->(f) RETURN u, f").await?;

// All stored in Arrow, zero-copy joins across models!
```

**10-100√ó faster than traditional ETL pipelines.**

---

## Real-World Use Cases

### Financial Services

| Challenge | Traditional | Pyralog |
|-----------|-------------|---------|
| **Trade processing** | Kafka ‚Üí Flink ‚Üí ClickHouse | Single platform |
| **Latency** | 500ms | 5ms (100√ó faster) |
| **Result** | Delayed risk calculations | Real-time compliance |

### E-commerce

| Challenge | Traditional | Pyralog |
|-----------|-------------|---------|
| **Inventory + Orders + Payments** | PostgreSQL (1K tx/sec) | 4M tx/sec |
| **Flash sales** | Crashes under load | Millions of concurrent buyers |
| **Result** | Lost revenue | Seamless scaling |

### Observability

| Challenge | Traditional | Pyralog |
|-----------|-------------|---------|
| **Stack** | Jaeger + Prometheus + ES | Unified platform |
| **Query speed** | 10-50√ó slower | Native Arrow analytics |
| **Result** | High costs, slow queries | 90% cost reduction |

### Real-Time ML

| Challenge | Traditional | Pyralog |
|-----------|-------------|---------|
| **Feature store** | Kafka + Redis + Custom | Built-in time-travel |
| **Freshness** | Minutes | <1ms |
| **Result** | Training/serving skew | Consistent features |

---

## Comparison with Existing Systems

| Feature | Kafka | TiKV | Databend | **Pyralog** |
|---------|-------|------|----------|-------------|
| **Write throughput** | 3.2M/s | 500K/s | N/A | **10M+/s** ‚úÖ |
| **Read throughput** | 8M/s | 1M/s | N/A | **45M+/s** ‚úÖ |
| **Transactions** | 100K/s | 500 tx/s | No | **4M+/s** ‚úÖ |
| **Coordinator ops** | 500K/s | 500K/s | N/A | **28B+/s** ‚úÖ |
| **Real-time SQL** | No | No | Yes | **Yes** ‚úÖ |
| **Exactly-once** | Complex | Yes | No | **Native** ‚úÖ |
| **Multi-model** | No | No | No | **6 models** ‚úÖ |
| **Time-travel** | No | Yes | Yes | **Yes** ‚úÖ |
| **Crypto verification** | No | No | No | **BLAKE3** ‚úÖ |
| **Language** | Java | Rust | Rust | **Rust** ‚úÖ |
| **Zero ETL** | No | No | No | **Yes** ‚úÖ |

---

## Why Now?

Three technology trends converge to make Pyralog possible:

### 1. Apache Arrow Has Matured
- Industry standard (Spark, Pandas 2.0, BigQuery, Snowflake)
- Rich ecosystem (DataFusion, Polars, DuckDB)
- Zero-copy interchange
- SIMD-optimized kernels

### 2. Rust is Production-Ready
- Memory safety without GC pauses
- Fearless concurrency
- Zero-cost abstractions
- 10-100√ó faster than Python/Java for data processing

### 3. Modern Hardware
- Fast NVMe storage (microsecond latency)
- High-bandwidth networks (100+ Gbps)
- Abundant CPU cores (128+ vCPUs)
- Large DRAM (1+ TB per node)

**Pyralog exploits all three to deliver unprecedented performance.**

---

## Two-Tier Architecture

### Obelisk Nodes (Pharaoh Network Layer)

**Purpose:** Lightweight coordination, ID generation, stateless routing

```
Characteristics:
  ‚Ä¢ Small (~10-50 MB memory each)
  ‚Ä¢ Stateless (just Obelisk Sequencer files)
  ‚Ä¢ Fast (millions of ops/sec per node)
  ‚Ä¢ No Raft (coordination-free!)
  ‚Ä¢ Scale independently
  
Responsibilities:
  ‚Ä¢ Generate Scarab IDs
  ‚Ä¢ Timestamp allocation (TSO)
  ‚Ä¢ Transaction coordination
  ‚Ä¢ Session management
  ‚Ä¢ Consumer group coordination
```

### Pyramid Nodes (Storage/Compute Layer)

**Purpose:** Data storage, consensus, query processing

```
Characteristics:
  ‚Ä¢ Larger (~10-100 GB memory each)
  ‚Ä¢ Stateful (LSM-Tree, Raft logs)
  ‚Ä¢ High throughput (100K+ writes/sec/partition)
  ‚Ä¢ Per-partition Raft consensus
  ‚Ä¢ Scale with data/partitions
  
Responsibilities:
  ‚Ä¢ Store data (Arrow + Parquet)
  ‚Ä¢ Execute queries (DataFusion)
  ‚Ä¢ Maintain consistency (Raft)
  ‚Ä¢ Replication (CopySets)
  ‚Ä¢ Compaction & cleanup
```

**Benefits of separation:**
- Scale coordination and storage independently
- Lighter failure domain for Obelisk nodes
- Pyramid nodes focus on data, not coordination
- Total cluster ops/sec = Obelisk ops + Pyramid ops

---

## Getting Started

### Installation

```bash
# Clone repository
git clone https://github.com/pyralog/pyralog
cd pyralog

# Build (requires Rust 1.70+)
cargo build --release

# Start local cluster
./target/release/pyralog-server --config cluster.toml
```

### Quick Examples

**Write data:**
```rust
use pyralog_client::PyralogClient;

let client = PyralogClient::connect("localhost:9092").await?;

// Simple write
client.produce("events", Record::new(
    Some(b"user-123".to_vec()),
    b"login".to_vec(),
)).await?;
```

**Query with SQL:**
```rust
// Real-time analytics
let results = client.sql("
    SELECT user_id, COUNT(*) as events
    FROM events
    WHERE timestamp > now() - INTERVAL '1 hour'
    GROUP BY user_id
    ORDER BY events DESC
    LIMIT 10
").await?;
```

**Transactions:**
```rust
// ACID across partitions
let tx = client.begin_transaction().await?;
tx.write("accounts", debit(account_a, 100)).await?;
tx.write("accounts", credit(account_b, 100)).await?;
tx.commit().await?;
```

---

## What's Next?

This is the first in a **30-post series** diving deep into Pyralog's architecture and implementation.

### Upcoming Posts

| Post | Topic | Focus |
|------|-------|-------|
| **02** | [üóø Obelisk Sequencer](02-obelisk-sequencer.md) | Persistent atomic counters |
| **03** | [‚òÄÔ∏è Pharaoh Network](03-pharaoh-network.md) | Coordination without bottlenecks |
| **04** | [28 Billion Ops/Sec](04-28-billion-ops.md) | Performance deep-dive |
| **05** | [Rust Infrastructure](05-rust-infrastructure.md) | Building with Rust |
| **06** | [Cryptographic Verification](06-cryptographic-verification.md) | BLAKE3 Merkle trees |
| **07** | [Multi-Model Database](07-multi-model-database.md) | Six data models in Arrow |
| **08** | [Batuta Language](08-batuta-language.md) | Category Theory queries |
| **09** | [Actor Concurrency](09-actor-concurrency.md) | Supervision trees |
| **10** | More coming soon! | Full series in blog/ |

---

## Community & Resources

### Documentation
- **GitHub**: [github.com/pyralog/pyralog](https://github.com/pyralog/pyralog)
- **Research Paper**: [PAPER.md](../PAPER.md) (1,774 lines)
- **Architecture**: [ARCHITECTURE.md](../ARCHITECTURE.md) (3,463 lines)
- **Total Docs**: 94K+ lines across 144 files

### Get Involved
- **Discord**: [discord.gg/pyralog](https://discord.gg/pyralog)
- **Issues**: [github.com/pyralog/pyralog/issues](https://github.com/pyralog/pyralog/issues)
- **Discussions**: [github.com/pyralog/pyralog/discussions](https://github.com/pyralog/pyralog/discussions)

### Looking For
- ‚úÖ **Early adopters** to test and provide feedback
- ‚úÖ **Contributors** to help build features
- ‚úÖ **Companies** interested in production deployments
- ‚úÖ **Researchers** in distributed systems, category theory, databases

### Contact
- **Email**: hello@pyralog.io
- **Enterprise**: enterprise@pyralog.io
- **Twitter/X**: [@pyralog](https://twitter.com/pyralog)

---

## Open Source & Licensing

**Licenses:**
- Code: **MIT-0** (no attribution required)
- Documentation: **CC0-1.0** (public domain)

**Philosophy:**
- Truly open source (no relicensing risk)
- Community-driven development
- Academic research friendly
- Commercial use encouraged

**Built with LLM assistance:**
- 77K+ lines of documentation created with Claude 3.5 Sonnet
- GraphMD workflow formalized collaboration process
- See [blog/28-graphmd.md](28-graphmd.md) for the story

---

**Author**: Pyralog Team  
**Published**: November 1, 2025  
**License**: MIT-0 (code) & CC0-1.0 (docs)  
**GitHub**: [github.com/pyralog/pyralog](https://github.com/pyralog/pyralog)

---

*Next: [The Obelisk Sequencer: A Novel Persistent Atomic Primitive ‚Üí](02-obelisk-sequencer.md)*
